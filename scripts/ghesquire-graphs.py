"""
Generate radius r (default r=1) graphs for Ghesquire dataset.
And make processed_data, note that samples with fluc values
as nan, inf measured by sasa, anm or nma were removed
"""

import os
import pickle as pkl
from functools import partial

import janitor
import networkx as nx

# Add in SASA scores generated by ghesquire-sasa.py
import numpy as np
import pandas as pd
from joblib import Parallel, delayed
from multipledispatch import dispatch
from proteingraph import read_pdb
from pyprojroot import here
from tqdm.auto import tqdm

from patch_gnn.data import load_ghesquire
from patch_gnn.graph import extract_neighborhood, get_node, met_position

pd.options.mode.use_inf_as_na = True  # this will set inf as nan, so that we can identify nan or inf together


@dispatch(float)
def split_delimiter(x, delimiter=";"):
    """Split delimiter helper function for floats."""
    return x


@dispatch(str)
def split_delimiter(x, delimiter=";"):
    """Split delimiter helper function for strings."""
    return x.split(delimiter)


# id for pdb files on data_store:2dd0e2bb-ab30-4983-89b0-89dffd78cb8f
models_path = here() / "data/ghesquire_2011/models"
built_models = [f.strip(".pdb") for f in os.listdir(models_path)]

protein_models = os.listdir(models_path)


failed_files = []


def load_model(model: str):
    """Safe loading of model in parallel computation."""
    try:
        m = read_pdb(models_path / f"{model}.pdb")
        return model, m
    except Exception as e:
        failed_files.append(model)
        print(e)


results = Parallel(n_jobs=-1)(delayed(load_model)(m) for m in built_models)

results = [r for r in results if r is not None]
results_accession_list = [result[0] for result in results]

data = load_ghesquire()
data["accession"] = data["accession"].fillna(method="ffill")

processed_data = (
    data.drop_duplicates(  # .dropna(subset=["accession"])
        subset=["accession", "end"]
    )
    .transform_column("isoforms", split_delimiter)
    .explode("isoforms")
    .transform_column("isoforms", partial(split_delimiter, delimiter=" ("))
    .transform_column("isoforms", lambda x: x[0] if isinstance(x, list) else x)
    .transform_column(
        "isoforms", lambda x: x.strip(" ") if isinstance(x, str) else x
    )
    .drop_duplicates("sequence")
    .join_apply(met_position, "met_position")
    .query("accession in @results_accession_list")
)


def get_met_graph_radius_n(
    r: int, results: list, processed_data: pd.DataFrame
):
    """
    Get Methionine graph of each pdb file that is within radius of r (<=r)
    :param r: radius, indicates the graph would include AA of <=r distance away from Met
    :param results: a list with output from function `load_model`
    :param processed_data: processed raw input dataset with Met location
    """
    met_graphs = dict()
    # tqdm load progress bar
    for accession, g in tqdm(results):
        row = processed_data.set_index("accession").loc[accession]
        pos = row["met_position"]
        seq = row["sequence"]
        try:
            metnode = get_node(g, pos)  # get met node
            met_g = extract_neighborhood(
                g, metnode, r
            )  # extract neighbors of met 1 degree away
            met_graphs[accession + "-" + seq] = met_g
        except Exception as e:
            print(e)
    return met_graphs


# add sasa, anm, nma df
sasa_pickle_path = here() / "data/ghesquire_2011/sasa.pkl"
anm_pickle_path = here() / "data/ghesquire_2011/ANM.pkl"
nma_pickle_path = here() / "data/ghesquire_2011/NMA.pkl"


with open(sasa_pickle_path, "rb") as f:
    sasa_dfs = pkl.load(f)

with open(anm_pickle_path, "rb") as f:
    anm_dfs = pkl.load(f)

with open(nma_pickle_path, "rb") as f:
    nma_dfs = pkl.load(f)

##### get a list of accession num to remove
##### because of the inf or nan values in fluc files (sasa, nma, anm)
accession_to_rm = []
count = 0
for acc, pos in zip(
    processed_data["accession"], processed_data["met_position"]
):
    # becuase of 'pd.options.mode.use_inf_as_na = True', nan and inf are all count as null
    if anm_dfs[acc].loc[:, "fluc"].isnull().any().sum() != 0:
        print("inf values")
        count += 1
        accession_to_rm.append(acc)
    elif nma_dfs[acc].loc[:, "fluc"].isnull().any().sum() != 0:
        print("inf values")
        count += 1
        accession_to_rm.append(acc)
    elif sasa_dfs["A6NEC2"].iloc[:, -6:].isnull().any().sum() != 0:
        count += 1
        accession_to_rm.append(acc)

print(
    f"{count} num of samples will be removed, the accession numbers are {accession_to_rm}"
)

# further drop the graphs with accession numbers in accession_to_rm
results = [result for result in results if result[0] not in accession_to_rm]
processed_data = processed_data.query("accession not in @accession_to_rm")

## extract neighbors of met 1 degree away
met_graphs = get_met_graph_radius_n(1, results, processed_data)
## add graph attributes
for acc_seq, g in tqdm(met_graphs.items()):
    accession = acc_seq.split("-")[0]
    sasa_df = sasa_dfs[accession]
    anm_df = anm_dfs[accession]
    nma_df = nma_dfs[accession]
    for n, d in g.nodes(data=True):
        res = d["residue_name"]
        num = d["residue_number"]
        sasa_info = (
            sasa_df.query("ResidNe == @res").query("ResidNr == @num").iloc[0]
        )
        g.nodes[n]["log_Phob/A^2"] = np.log(sasa_info["Phob/A^2"])
        g.nodes[n]["log_Phil/A^2"] = np.log(sasa_info["Phil/A^2"])
        g.nodes[n]["log_SASA/A^2"] = np.log(sasa_info["SASA/A^2"])
        g.nodes[n]["log_N(overl)"] = np.log(sasa_info["N(overl)"])
        ## add anm and nma
        anm_info = (
            anm_df.query("resids == @res").query("resnos == @num").iloc[0]
        )
        g.nodes[n]["anm"] = anm_info["fluc"]
        nma_info = (
            nma_df.query("resids == @res").query("resnos == @num").iloc[0]
        )
        g.nodes[n]["nma"] = nma_info["fluc"]

# further process data
processed_data = processed_data.query(
    "`accession-sequence` in @met_graphs.keys()"
).query("ox_fwd_logit < 2.0")

graph_pickle_dir = here() / "data/ghesquire_2011"
graph_pickle_dir.mkdir(exist_ok=True, parents=True)

with open(graph_pickle_dir / "graphs.pkl", "wb") as f:
    pkl.dump(met_graphs, f)

with open(here() / "data/ghesquire_2011/processed_data.pkl", "wb") as f:
    pkl.dump(processed_data, f)
