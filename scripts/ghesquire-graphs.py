"""Generate radius 1 graphs for Ghesquire dataset."""

import os
import pickle as pkl
from functools import partial

import janitor
import networkx as nx

# Add in SASA scores generated by ghesquire-sasa.py
import numpy as np
import pandas as pd
from joblib import Parallel, delayed
from multipledispatch import dispatch
from proteingraph import read_pdb
from pyprojroot import here
from tqdm.auto import tqdm

from patch_gnn.data import load_ghesquire
from patch_gnn.graph import extract_neighborhood, get_node, met_position

#id for pdb files on data_store:2dd0e2bb-ab30-4983-89b0-89dffd78cb8f
models_path = here() / "data/ghesquire_2011/models"  
protein_models = os.listdir(models_path)

data = load_ghesquire()
data["accession"] = data["accession"].fillna(method="ffill")


@dispatch(float)
def split_delimiter(x, delimiter=";"):
    """Split delimiter helper function for floats."""
    return x


@dispatch(str)
def split_delimiter(x, delimiter=";"):
    """Split delimiter helper function for strings."""
    return x.split(delimiter)


processed_data = (
    data.drop_duplicates(  # .dropna(subset=["accession"])
        subset=["accession", "end"]
    )
    .transform_column("isoforms", split_delimiter)
    .explode("isoforms")
    .transform_column("isoforms", partial(split_delimiter, delimiter=" ("))
    .transform_column("isoforms", lambda x: x[0] if isinstance(x, list) else x)
    .transform_column(
        "isoforms", lambda x: x.strip(" ") if isinstance(x, str) else x
    )
    .drop_duplicates("sequence")
    .join_apply(met_position, "met_position")
)


built_models = [f.strip(".pdb") for f in os.listdir(models_path)]

failed_files = []
def load_model(model: str):
    """Safe loading of model in parallel computation."""
    try:
        m = read_pdb(models_path / f"{model}.pdb")
        return model, m
    except Exception as e:
        failed_files.append(model)
        print(e)


results = Parallel(n_jobs=-1)(delayed(load_model)(m) for m in built_models)

results = [r for r in results if r is not None]

met_graphs = dict()

# tqdm load progress bar
for accession, g in tqdm(results):
    row = processed_data.set_index("accession").loc[accession]
    pos = row["met_position"]
    seq = row["sequence"]

    try:
        metnode = get_node(g, pos)  # get met node
        met_g = extract_neighborhood(
            g, metnode, 1
        )  # extract neighbors of met 1 degree away
        met_graphs[accession + "-" + seq] = met_g
    except Exception as e:
        print(e)


sasa_pickle_path = here() / "data/ghesquire_2011/sasa.pkl"

with open(sasa_pickle_path, "rb") as f:
    sasa_dfs = pkl.load(f)

for acc_seq, g in tqdm(met_graphs.items()):
    accession = acc_seq.split("-")[0]
    sasa_df = sasa_dfs[accession]
    for n, d in g.nodes(data=True):
        res = d["residue_name"]
        num = d["residue_number"]
        sasa_info = (
            sasa_df.query("ResidNe == @res").query("ResidNr == @num").iloc[0]
        )
        g.nodes[n]["log_Phob/A^2"] = np.log(sasa_info["Phob/A^2"])
        g.nodes[n]["log_Phil/A^2"] = np.log(sasa_info["Phil/A^2"])
        g.nodes[n]["log_SASA/A^2"] = np.log(sasa_info["SASA/A^2"])
        g.nodes[n]["log_N(overl)"] = np.log(sasa_info["N(overl)"])

graph_pickle_dir = here() / "data/ghesquire_2011"
graph_pickle_dir.mkdir(exist_ok=True, parents=True)

with open(graph_pickle_dir / "graphs.pkl", "wb") as f:
    pkl.dump(met_graphs, f)
