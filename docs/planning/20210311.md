# Notes from today's hack

## Present

- Kannan Sankar
- Jiayi Cox
- Mei Xiao
- Eric Ma

## Things done

- GAT interpretability done.
- Discussed final notes before EM heads off for paternity leave.
- Made slides comparing attention maps for low and high oxidizing methionines.

## Conclusions on Ghesquire 2011 dataset

Sequence is not sufficient to predict MetOx liability
(as evident from poor performance of UniRep model on this dataset)

Interpretation of attention GNNs reveals that
it is capturing hydrophobicity of neighbors as important factor

Patch-GNN models offer only a slightly better predictive ability
(expl variance of ~0.3+) than simpler linear or random forest models
with hand-crafted desriptors like SASA, # overlaps, etc. for this dataset.
This suggests fundamental deficiencies in the quality of this external dataset
that can confuscate model efficiency.
Some factors we thought of:

a. biases in type of proteins, e.g. membrane, ribosomal, etc.
b. structure incompleteness of the homology models leading to incorrectly calculated exposures of Met residues
c. other residue modifications like glycosylations on proteins are not accounted when using modelled structure.


## Future work

Possible next steps:

1. Separate implementation of GraphAttention and make a similar one, the goal is to put more trainable weights to increase the model interpretability.
2. Find another global or local attention, this requires better understanding of the property related to protein structure and the task
3. More comprehensively push the sequence-based models to verify that sequence-based models aren't good.
