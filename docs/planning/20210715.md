## Present

- Kannan Sankar
- Jiayi Cox
- Mei Xiao


## Things done
1. Removed samples with inf or nan fluc values globally, added sample QC process in ghesquire-graph.py so that one can load processed_data file directly without needing to QC it
2. Finished the graph attention neural network, performance tested
	a. zero entries were produced when element wise multiply attention to adjacency matrix, one can apply softmax on zero entries, but it would encounter 0 division in practice
	b. in the end, we resolved the attention machenism by replacing leaky_relu to hard_tanh, so that the activation output will always be [-1,1], then we apply the element wise adjacency mask to get the actual attention paid from node to node where there is an edge. Instead of having node to node in row-wise sum up to 1, we interpret each row as how much attention row node pays to column nodes
3.Finished a notebook for graph attention neural network so one can visualize the attention met has on surrounding residues
4. Like before, observed oscillating loss in the later epochs, this could be due to
	a.limited input samples, not enough samples to cover the entire continuous error space
	b.relatively high learning rate, so that the training could not converge
		i.experiment learning rate decay? This part has already been addressed by using `adam` optimizer
5.Implemented auto-hyperparameter search using `optuna` package
	a.This makes things really convenient since we will only define a range of hyperparameters, and let optuna to decide which one gives the best training performance
	b.Verified that new trials (n_trials is defined in study.optimize function) does not carry over with results from previous trial (in other words, trials do not change states in other trials, each trial is totally independent). Below is an example on simulated data that with different trials, using the same hyperparameters returns the same error values.
	c. Used random search for hyperparaemter search, set seed to make it reproducible
6. contacted Zach to learn Dask first
dask: https://bitbucket.prd.nibr.novartis.net/projects/SDA/repos/nibr-dask/browse
Lessons learned - Dask
Best practices for Dask on the HPC environment
Create a dask jobqueue cluster (for our HPC environment)
need to estimate how much memory is required before running it
if the job pops and and disppear, it has problem, check submission logs in ~
7. Added other measures for training accuracy:
	a. Added mse, for the same dataset, MSE is the training loss we are trying to optimize, the training loss does not always closely corrlate with variance explained, so using MSE can give us a better understanding of how Optuna is doing
	b. the diff between variance explained and R square is that: variance explained is 1 - Var(y-y_predicted)/Var(y), R square is 1 - Var(y-y_predicted)/Var(y-y_mean)


# To do:
Meet next time to make some slides about the methods used for this project
