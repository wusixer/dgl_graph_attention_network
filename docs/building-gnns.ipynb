{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will show how to build a protein patch GNN using the software library we have built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading proteins as graph objects in memory\n",
    "\n",
    "The first thing we do is to load a PDB file into memory as a NetworkX graph object. The tool that we use is called `proteingraph`, and is installable directly from PyPI. The next generation version of `proteingraph` is called `graphein`, and is designed to handle the conversion of multiple types of biological data into its graph representation.\n",
    "\n",
    "In any case, to load a PDB file of a protein structure into a NetworkX graph object, we use the `read_pdb` function from `proteingraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteingraph import read_pdb\n",
    "from pyprojroot import here\n",
    "\n",
    "hiv_graph = read_pdb(here() / \"data/hiv1_homology_model.pdb\")\n",
    "hiv_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hiv_graph` is a NetworkX graph object, as you can see from above, and as such will work with the NetworkX API seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiv_graph.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating protein patches from a NetworkX graph object\n",
    "\n",
    "Our `patch_gnn` package contains the functions necessary to generate graph \"patches\" from a graph. Basically, these graph patches are subgraphs of the larger protein graph, and are defined for a radius around every amino acid in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_gnn.graph import generate_patches\n",
    "graph_patches = generate_patches(hiv_graph, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visulaize a few of them to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx.draw(graph_patches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(graph_patches[-10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the graph object into array format\n",
    "\n",
    "Data preparation is key for us to be able to use the neural network layers defined in `patch_gnn`. In particular, we need to be able to describe amino acids according to numerical molecular descriptors. This will give us the \"node feature\" matrix that exists for each patch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPASY molecular descriptors\n",
    "\n",
    "We have downloaded molecular descriptors from EXPASY, which gives us a large table of 61 descriptors for each of the 20 standard amino acids. From this, we will generate a \"node feature matrix\", in which each row is one amino acid in the protein patch, and each column is a particular molecular descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "aa_feats = pd.read_csv(here() / \"data/amino_acid_properties.csv\", index_col=0)\n",
    "aa_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating graph feature tensors\n",
    "\n",
    "We define a function that will be compatible with `patch_gnn`'s machinery to annotate every amino acid and get back a human-friendly, inspectable dataframe version of the node feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_amino_acid(n, d, aa_feats: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Featurize a single amino acid.\n",
    "    \n",
    "    :param n: Graph node.\n",
    "    :param d: Graph node attributes.\n",
    "    :param aa_feats: Dataframe containing amino acid features.\n",
    "    \"\"\"\n",
    "    aa = d[\"residue_name\"]\n",
    "    feats = pd.Series(aa_feats[aa], name=n)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we collect the node featurization functions into a list.\n",
    "We must use `partial` to enure that each function's signature is limited to `n, d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "feature_funcs = [partial(featurize_amino_acid, aa_feats=aa_feats)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we stack the feature tensors for all graph patches (they are individual graph objects themselves) together using the `stack_feature_tensors` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_gnn.graph import stack_feature_tensors\n",
    "Fs = stack_feature_tensors(graph_patches, funcs=feature_funcs)\n",
    "Fs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating graph adjacency tensors\n",
    "\n",
    "The graph adjacency tensors form the diffusion matrix that is used for the message passing operation in graph neural networks. We are free to define any kind of diffusion matrix, as long as they are square matrices.\n",
    "\n",
    "The semantic meaning of each diffusion matrix may vary. For example, the adjacency matrix + identity matrix put together serves to express the idea of adding \"myself + my neighbors\" together, while the adjacency matrix alone expresses \"give me my neighbors' messages\".\n",
    "\n",
    "Knowing what diffusion matrices to use is likely an area of active research; with `patch_gnn`, we simply assume that there are a ton of them that can be used, and that we linearly combine their results together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "from patch_gnn.graph import (\n",
    "    identity_matrix, \n",
    "    adjacency_matrix, \n",
    "    laplacian_matrix,\n",
    "    to_adjacency_xarray,\n",
    "    stack_adjacency_tensors\n",
    ")\n",
    "\n",
    "adjacency_funcs = []\n",
    "for i in range(3):\n",
    "    adjacency_funcs.append(partial(adjacency_matrix, power=i, name=f\"adjacency_{i}\"))\n",
    "adjacency_funcs.extend(\n",
    "    [\n",
    "        identity_matrix,\n",
    "        laplacian_matrix,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As = stack_adjacency_tensors(graph_patches, funcs=adjacency_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network models\n",
    "\n",
    "We can now embark on writing neural network models that take both the `As` and `Fs` stacked up together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "\n",
    "Firstly, we might want a custom graph embedding layer.\n",
    "\n",
    "Here, what we do is stack together a message passing layer,\n",
    "followed by Dense-Simgoid transformation,\n",
    "followed by a graph summation op,\n",
    "then another linear projection to 256 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import stax\n",
    "from patch_gnn.layers import MessagePassing, GraphSummation\n",
    "\n",
    "def CustomGraphEmbedding(n_output: int):\n",
    "    \"\"\"Return an embedding of a graph in n_output dimensions.\"\"\"\n",
    "    init_fun, apply_fun = stax.serial(\n",
    "        MessagePassing(),\n",
    "        stax.Dense(2048),\n",
    "        stax.Sigmoid,\n",
    "        GraphSummation(),\n",
    "        stax.Dense(n_output),\n",
    "    )\n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to verify that everything works properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_init_fun, embedding_apply_fun = CustomGraphEmbedding(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top models\n",
    "\n",
    "Then, we might want to stack it together with a Linear regression model (for unbounded prediction) or logistic regression layer (for binary classification problems). Here's two examples that we can compose together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.random import PRNGKey\n",
    "\n",
    "def LinearRegression(num_outputs):\n",
    "    \"\"\"Linear regression layer.\"\"\"\n",
    "    init_fun, apply_fun = stax.serial(\n",
    "        stax.Dense(num_outputs),\n",
    "    )\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "def LogisticRegression(num_outputs):\n",
    "    \"\"\"Logistic regression layer.\"\"\"\n",
    "    init_fun, apply_fun = stax.serial(\n",
    "        stax.Dense(num_outputs),\n",
    "        stax.Softmax,\n",
    "    )\n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing a full model\n",
    "\n",
    "Let's now compose the full model together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init_fun, model_apply_fun = stax.serial(\n",
    "    CustomGraphEmbedding(256),\n",
    "    LinearRegression(1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And while we're at it, initialize a random set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape, params = model_init_fun(PRNGKey(42), input_shape=(*Fs[0].shape, As[0].shape[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "\n",
    "We test the model to make sure data can be passed through it and that we get the right shape at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "inputs = (As, Fs)\n",
    "out = vmap(partial(model_apply_fun, params))(inputs)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the embedding\n",
    "\n",
    "We do the same with the embedding, this time passing in the first parameter element (since our model is composed of two high-level elements that themselves are potentially nested)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = vmap(partial(embedding_apply_fun, params[0]))(inputs)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-driving a learning task\n",
    "\n",
    "We're now going to set up a dummy learning task. In this learning task, we'll see whether we can learn a random number mapping for each graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "outputs = random.normal(PRNGKey(42), shape=(len(graph_patches), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to train a model to predict (ahem, memorize) these numbers from the graph.\n",
    "\n",
    "We start by test-driving the loss, meticulously making sure that it returns a scalar only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_gnn.training import mseloss\n",
    "from jax import grad\n",
    "\n",
    "mseloss(params, model_apply_fun, inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we write the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.optimizers import adam\n",
    "from patch_gnn.training import mseloss\n",
    "from jax import grad\n",
    "\n",
    "dmseloss = grad(mseloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(i, state, dloss_fun, apply_fun, update_fun, get_params, inputs, outputs):\n",
    "    params = get_params(state)\n",
    "    g = dloss_fun(params, apply_fun, inputs, outputs)\n",
    "    state = update_fun(i, g, state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "from typing import Tuple\n",
    "\n",
    "init, update, get_params = adam(step_size=1e-3)\n",
    "get_params = jit(get_params)\n",
    "state = init(params)\n",
    "\n",
    "random_training_step = partial(step, dloss_fun=dmseloss, apply_fun=model_apply_fun, update_fun=update, get_params=get_params, inputs=inputs, outputs=outputs)\n",
    "random_training_step = jit(random_training_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For-loop based training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "for i in tqdm(range(100)):\n",
    "    state = random_training_step(i, state, inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params = get_params(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mseloss(final_params, model_apply_fun, inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `lax.scan`-based training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax\n",
    "\n",
    "\n",
    "def make_step_scannable(get_params_func, dloss_func, model_func, update_func, inputs, outputs):\n",
    "    def inner(previous_state, iteration):\n",
    "        new_state = step(\n",
    "            i=iteration,\n",
    "            state=previous_state,\n",
    "            get_params=get_params_func,\n",
    "            dloss_fun=dloss_func,\n",
    "            apply_fun=model_func,\n",
    "            update_fun=update_func,\n",
    "            inputs=inputs,\n",
    "            outputs=outputs,\n",
    "        )\n",
    "        return new_state, previous_state\n",
    "\n",
    "    return inner\n",
    "from jax.experimental.optimizers import adam\n",
    "\n",
    "adam_init, adam_update, adam_get_params = adam(0.005)\n",
    "\n",
    "step_scannable = make_step_scannable(adam_get_params, dmseloss, model_apply_fun, adam_update, inputs, outputs)\n",
    "\n",
    "initial_state = adam_init(params)\n",
    "\n",
    "final_params, history = lax.scan(step_scannable, initial_state, np.arange(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patch-gnn",
   "language": "python",
   "name": "patch-gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
