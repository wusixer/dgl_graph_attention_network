{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax.numpy as np\n",
    "from jax import lax, nn, random, vmap\n",
    "from jax._src.nn.functions import normalize\n",
    "from jax.experimental import stax\n",
    "from jax.nn import sigmoid\n",
    "from jax.nn.initializers import glorot_normal, normal\n",
    "from jax.random import normal as norm\n",
    "from jax import lax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from patch_gnn.data import load_ghesquire\n",
    "import pandas as pd\n",
    "from pyprojroot import here\n",
    "import pickle as pkl\n",
    "from patch_gnn.splitting import train_test_split\n",
    "from jax import random\n",
    "from patch_gnn.seqops import one_hot\n",
    "from patch_gnn.unirep import unirep_reps\n",
    "from patch_gnn.graph import graph_tensors\n",
    "from patch_gnn.models import MPNN, DeepMPNN\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import explained_variance_score as evs\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle as pkl\n",
    "from patch_gnn.graph import met_position\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset, split into train and test, then one hot encode it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of train_df and test_df are (258, 18), (111, 18)\n",
      "the shape of train_oh and test_oh are (258, 1050), (111, 1050)\n"
     ]
    }
   ],
   "source": [
    "# load graphs\n",
    "graph_pickle_path = here() / \"data/ghesquire_2011/graphs.pkl\"\n",
    "with open(graph_pickle_path, \"rb\") as f:\n",
    "    graphs = pkl.load(f)\n",
    "\n",
    "# load data\n",
    "data = load_ghesquire()\n",
    "\n",
    "# filter data based on graphs.keys() and other metrics\n",
    "filtered = (\n",
    "   data\n",
    "   .query(\"`accession-sequence` in @graphs.keys()\")\n",
    "   .query(\"ox_fwd_logit < 2.0\")\n",
    "   .join_apply(met_position, \"met_position\")\n",
    ")\n",
    "\n",
    "# split data into train and testing\n",
    "key = random.PRNGKey(490)\n",
    "train_df, test_df = train_test_split(key, filtered) # 70% training, 30% testing\n",
    "print(f\"the shape of train_df and test_df are {train_df.shape}, {test_df.shape}\")\n",
    "\n",
    "# pad sequence to 50 length and one hot encode it\n",
    "padding_length = 50\n",
    "train_oh = one_hot(train_df, padding_length) \n",
    "test_oh = one_hot(test_df,padding_length)\n",
    "print(f\"the shape of train_oh and test_oh are {train_oh.shape}, {test_oh.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reshape data to fit LSTM and get target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(258, 50, 21) (111, 50, 21)\n"
     ]
    }
   ],
   "source": [
    "lstm_train_oh = train_oh.reshape(train_oh.shape[0], padding_length, 21)\n",
    "lstm_test_oh = test_oh.reshape(test_oh.shape[0], padding_length, 21)\n",
    "print(lstm_train_oh.shape, lstm_test_oh.shape)\n",
    "train_target = train_df['ox_fwd_logit'].values\n",
    "test_target = test_df['ox_fwd_logit'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 50, 21), (1,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_train_oh[0:1,:,:].shape, train_target[0:1,].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM based on GRU's case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AAEmbedding(embedding_dims: int , E_init=glorot_normal(), **kwargs):\n",
    "    \"\"\"\n",
    "    Initial n-dimensional embedding of each amino-acid\n",
    "    \"\"\"\n",
    "\n",
    "    def init_fun(rng, input_shape):\n",
    "        \"\"\"\n",
    "        Generates the inital AA embedding matrix.\n",
    "        `input_shape`:\n",
    "            one-hot encoded AA sequence -> (n_aa, n_unique_aa)\n",
    "        `output_dims`:\n",
    "            embedded sequence -> (n_aa, embedding_dims)\n",
    "        `emb_matrix`:\n",
    "            embedding matrix -> (n_unique_aa, embedding_dims)\n",
    "        \"\"\"\n",
    "        emb_matrix = E_init(rng, (input_shape[-1], embedding_dims))\n",
    "        output_dims = (-1, embedding_dims)\n",
    "\n",
    "        return output_dims, emb_matrix\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Embed a single AA sequence\n",
    "        \"\"\"\n",
    "        emb_matrix = params\n",
    "        # (n_aa, n_unique_aa) * (n_unique_aa, embedding_dims) => (n_aa, embedding_dims) # noqa: E501\n",
    "        return np.matmul(inputs, emb_matrix)\n",
    "\n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(out_dim, W_init=glorot_normal(), b_init=normal()):\n",
    "    \"\"\"\n",
    "    one directional LSTM, see math here https://d2l.ai/chapter_recurrent-modern/lstm.html\n",
    "    :params out_dim: number of output neurons associated with an input of a single time point\n",
    "    \n",
    "    \"\"\"\n",
    "    def init_fun(rng, input_shape):\n",
    "        \"\"\"\n",
    "        initialize LSTM layer for stax\n",
    "        :param rng: The PRNGKey (from JAX) for random number generation _reproducibility_.\n",
    "        :params input_shape: (num_time_steps/n_letters, embeddings)\n",
    "        \"\"\"\n",
    "        # initial hidden state and memory state\n",
    "        hidden = b_init(rng, (1, out_dim)) # denote by H in the formula #b_init(rng, (input_shape[0], out_dim)), instead None can be any number\n",
    "        memory = b_init(rng, (1, out_dim)) # denote by C in the formula #b_init(rng, (input_shape[0], out_dim))\n",
    "        # input gate\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        input_W, input_U, input_b = (\n",
    "            W_init(k1, (input_shape[-1], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),\n",
    "        )\n",
    "        # forget gate\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        forget_W, forget_U, forget_b = (\n",
    "            W_init(k1, (input_shape[-1], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),\n",
    "        )\n",
    "        # output gate\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        output_W, output_U, output_b = (\n",
    "            W_init(k1, (input_shape[-1], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),\n",
    "        )\n",
    "        # current memory \n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        candidate_m_W, candidate_m_U, candidate_m_b = (\n",
    "            W_init(k1, (input_shape[-1], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),\n",
    "        )\n",
    "        \n",
    "        # Input dim 0 - batch dimension, 1 - time dimension (before scan moveaxis)\n",
    "        output_shape = (input_shape[0], out_dim )#,input_shape[0],\n",
    "        return (output_shape,\n",
    "               ((hidden, memory),\n",
    "               (input_W, input_U, input_b),\n",
    "               (forget_W, forget_U, forget_b),\n",
    "               (output_W, output_U, output_b),\n",
    "               (candidate_m_W, candidate_m_U, candidate_m_b)),) # this tuple is (output_shape, params)\n",
    "    \n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        \"\"\" Loop over the time steps of the input sequence \"\"\"\n",
    "        h_0, m_0 = params[0] # initial hidden and memory\n",
    "        \n",
    "        def apply_fun_scan(params, carry, inp):\n",
    "            \"\"\" \n",
    "            Perform single step update of the network\n",
    "            carry: a tuple with (hidden, memory)\n",
    "            :param inp: of shape \n",
    "            \"\"\"\n",
    "            (hidden, memory) = carry\n",
    "            (i,j), (input_W, input_U, input_b), (forget_W, forget_U, forget_b), (\n",
    "                output_W, output_U, output_b),(\n",
    "                candidate_m_W, candidate_m_U, candidate_m_b) = params\n",
    "            # shape annotation: (1,embedding)*(embedding, outdim) +(1, outdim)*(outdim, outdim) => (1, outdim)\n",
    "            input_gate = sigmoid(np.dot(inp, input_W) +\n",
    "                                  np.dot(hidden, input_U) + input_b)\n",
    "            # shape annotation: (1,embedding)*(embedding, outdim) +(1, outdim)*(outdim, outdim)=> (1, outdim)\n",
    "            forget_gate = sigmoid(np.dot(inp, forget_W) +\n",
    "                                 np.dot(hidden, forget_U) + forget_b)\n",
    "            # shape annotation: (1,embedding)*(embedding, outdim) +(1, outdim)*(outdim, outdim)=> (1, outdim)\n",
    "            output_gate = sigmoid(np.dot(inp, output_W) +\n",
    "                                 np.dot(hidden, output_U) + output_b)\n",
    "            # shape annotation: (1,embedding)*(embedding, outdim) +(1, outdim)*(outdim, outdim)=> (1, outdim)\n",
    "            candidate_memory = np.tanh(np.dot(inp, candidate_m_W) +\n",
    "                                 np.dot(hidden, candidate_m_U) + candidate_m_b)\n",
    "            # shape annotation: (1,outdim)@(1,outdim) + 1,outdim)@(1,outdim)=> (1, outdim)\n",
    "            current_memory = np.multiply(forget_gate, memory) + np.multiply(input_gate, candidate_memory)\n",
    "            # shape annotation: (1,outdim)@(1, outdim)=> (1, outdim)\n",
    "            current_hidden = np.multiply(output_gate, np.tanh(current_memory))\n",
    "            hidden = current_hidden\n",
    "            memory = current_memory\n",
    "\n",
    "            return (hidden, memory), hidden\n",
    "\n",
    "        f = partial(apply_fun_scan, params) # this is a function\n",
    "        (h_final, m_final), output = lax.scan(f, init=(h_0, m_0), xs=inputs)\n",
    "        \n",
    "        \n",
    "        return (h_final, m_final), output\n",
    "\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### next step we want to generate some inputs for LSTM layer for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import value_and_grad, jit\n",
    "from jax.experimental import stax\n",
    "from patch_gnn.training import mseloss\n",
    "from jax.experimental.optimizers import adam\n",
    "\n",
    "# Initialize the network and perform a forward pass\n",
    "vanilla_lstm_init_fun, vanilla_lstm_apply_fun = stax.serial(\n",
    "    AAEmbedding(64), \n",
    "    stax.Sigmoid, \n",
    "    LSTM(32),\n",
    "    stax.Dense(64),\n",
    "    stax.Dense(1))#Dense(lstm_train_oh.shape[0])\n",
    "#_, params = dense_lstm_init_fun(key, (lstm_train_oh.shape[0], padding_length, 21))\n",
    "_, params = vanilla_lstm_init_fun(key,  (padding_length, 21))\n",
    "\n",
    "#def mse_loss(params, inputs, targets):\n",
    "#    \"\"\" Calculate the Mean Squared Error Prediction Loss. \"\"\"\n",
    "#    preds = vanilla_lstm_apply_fun(params, inputs)\n",
    "#    return np.mean((preds - targets)**2)\n",
    "# the following update function is the same as step function in patch_gnn.training\n",
    "def fit(\n",
    "    #sequences: Iterable[str],\n",
    "    num_epochs: int,\n",
    "    model_func: Callable = vanilla_lstm_apply_fun,\n",
    "    params: Any = None,\n",
    "    batch_size: int = 25,\n",
    "    step_size: float = 0.0001,\n",
    "    #holdout_seqs: Optional[Iterable[str]] = None,\n",
    "    #proj_name: str = \"temp\",\n",
    "    #epochs_per_print: int = 1,\n",
    "    backend: str = \"cpu\",\n",
    "):\n",
    "    @jit\n",
    "    def step(i, x, y, state):\n",
    "        \"\"\" Perform one timestep forward pass, calculate the MSE & perform a SGD step. \"\"\"\n",
    "        params = get_params(state)\n",
    "        g = grad(partial(loss, model_func))(params, x, y)\n",
    "        state = update(i, g, state)\n",
    "        return state\n",
    "\n",
    "    init, update, get_params = adam(step_size=step_size)\n",
    "    get_params = jit(get_params)\n",
    "    state = init(params)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "    for batch_idx in range(lstm_train_oh.shape[0]):\n",
    "        x_in = lstm_train_oh[batch_idx, :, :]\n",
    "        y = train_target[batch_idx:batch_idx+1, ]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.optimizers import adam\n",
    "# Defining an optimizer in Jax\n",
    "step_size = 1e-3\n",
    "opt_init, opt_update, get_params = adam(step_size)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dot requires ndarray or scalar arguments, got <class 'tuple'> at position 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFilteredStackTrace\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dc3f693b4b7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2eaab1cafc43>\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(params, inputs, targets)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m\"\"\" Calculate the Mean Squared Error Prediction Loss. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvanilla_lstm_apply_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/experimental/stax.py\u001b[0m in \u001b[0;36mapply_fun\u001b[0;34m(params, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_funs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/experimental/stax.py\u001b[0m in \u001b[0;36mapply_fun\u001b[0;34m(params, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0minit_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, precision)\u001b[0m\n\u001b[1;32m   3722\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3723\u001b[0;31m   \u001b[0m_check_arraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3724\u001b[0m   \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_promote_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_check_arraylike\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{} requires ndarray or scalar arguments, got {} at position {}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFilteredStackTrace\u001b[0m: TypeError: dot requires ndarray or scalar arguments, got <class 'tuple'> at position 0.\n\nThe stack trace above excludes JAX-internal frames.\nThe following is the original exception that occurred, unmodified.\n\n--------------------",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dc3f693b4b7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#params, opt_state, loss = update(params, x_in, y, opt_state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreraise_with_filtered_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_under_reraiser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/api.py\u001b[0m in \u001b[0;36mvalue_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_check_input_dtype_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholomorphic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdyn_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m       \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp_py\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_partial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdyn_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m       \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_partial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdyn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/api.py\u001b[0m in \u001b[0;36m_vjp\u001b[0;34m(fun, has_aux, *primals)\u001b[0m\n\u001b[1;32m   1876\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m     \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun_nokwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1878\u001b[0;31m     \u001b[0mout_primal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_vjp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1879\u001b[0m     \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(traceable, primals, has_aux)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mlinearize\u001b[0;34m(traceable, *primals, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m   \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_to_jaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m   \u001b[0mout_primals_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tangents_pvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_primal_pval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout_primal_pval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_primals_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr\u001b[0;34m(fun, pvals, instantiate)\u001b[0m\n\u001b[1;32m    496\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJaxprTrace\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_to_subjaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstantiate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2eaab1cafc43>\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(params, inputs, targets)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m\"\"\" Calculate the Mean Squared Error Prediction Loss. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvanilla_lstm_apply_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# the following update function is the same as step function in patch_gnn.training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/experimental/stax.py\u001b[0m in \u001b[0;36mapply_fun\u001b[0;34m(params, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mrngs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlayers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_funs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0minit_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/experimental/stax.py\u001b[0m in \u001b[0;36mapply_fun\u001b[0;34m(params, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mapply_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0minit_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, precision)\u001b[0m\n\u001b[1;32m   3721\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_wraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlax_description\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_PRECISION_DOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3722\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3723\u001b[0;31m   \u001b[0m_check_arraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3724\u001b[0m   \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_promote_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3725\u001b[0m   \u001b[0ma_ndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/patch-gnn/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_check_arraylike\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    303\u001b[0m                     if not _arraylike(arg))\n\u001b[1;32m    304\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{} requires ndarray or scalar arguments, got {} at position {}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_no_float0s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: dot requires ndarray or scalar arguments, got <class 'tuple'> at position 0."
     ]
    }
   ],
   "source": [
    "# Loop over the training epochs\n",
    "loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx in range(lstm_train_oh.shape[0]):\n",
    "        x_in = lstm_train_oh[batch_idx, :, :]\n",
    "        y = train_target[batch_idx:batch_idx+1, ]\n",
    "        print(type(x_in), type(y))\n",
    "        loss, grads = value_and_grad(mse_loss)(params, x_in, y)\n",
    "        print(loss, grads)\n",
    "        #params, opt_state, loss = update(params, x_in, y, opt_state)\n",
    "        #train_loss_log.append(loss)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### below part is what we want to do ultimately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import stax\n",
    "from jax.experimental.optimizers import adam\n",
    "class VanillaLSTM:\n",
    "    \"\"\"Vanilla shallow LSTM model in sklearn-compatible format.\n",
    "\n",
    "    Forward direction LSTM + linear regression on top.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        one_hot_encoded_shape,\n",
    "        #node_feature_shape,\n",
    "        #num_adjacency,\n",
    "        num_training_steps: int = 100,\n",
    "        optimizer_step_size=1e-5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param one_hot_encoded_shape: (batch_size, timestep, embedding), timestep is the padding_length\n",
    "        \n",
    "        \"\"\"\n",
    "        model_init_fun, model_apply_fun = stax.serial(\n",
    "            Dense(64), \n",
    "            stax.Sigmoid, \n",
    "            LSTM(32), \n",
    "            Dense(1)\n",
    "        )\n",
    "        self.model_apply_fun = model_apply_fun\n",
    "\n",
    "        self.optimizer = adam(step_size=optimizer_step_size)\n",
    "\n",
    "        output_shape, params = model_init_fun(\n",
    "            PRNGKey(42), input_shape=(*one_hot_encoded_shape)\n",
    "        )\n",
    "\n",
    "        self.params = params\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.state_history = []\n",
    "        self.loss_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit model.\n",
    "\n",
    "        :param X: tuple(timestep, embedding)\n",
    "        :param y: vector(values to predict)\n",
    "        \"\"\"\n",
    "        if len(y.shape) == 1:\n",
    "            y = np.reshape(y, (-1, 1))\n",
    "        init, update, get_params = self.optimizer\n",
    "        training_step = partial(\n",
    "            step,\n",
    "            loss_fun=mseloss,\n",
    "            apply_fun=self.model_apply_fun,\n",
    "            update_fun=update,\n",
    "            get_params=get_params,\n",
    "            inputs=X,\n",
    "            outputs=y,\n",
    "        )\n",
    "        training_step = jit(training_step)\n",
    "\n",
    "        state = init(self.params)\n",
    "\n",
    "        for i in tqdm(range(self.num_training_steps)):\n",
    "            state, loss = training_step(i, state)\n",
    "            self.state_history.append(state)\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "        self.params = get_params(state)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, checkpoint: int = None):\n",
    "        \"\"\"\n",
    "        predict\n",
    "        :param X: tuple(adjacency, node_features)\n",
    "        \"\"\"\n",
    "        params = self.params\n",
    "        if checkpoint:\n",
    "            _, _, get_params = self.optimizer\n",
    "            params = get_params(self.state_history[checkpoint])\n",
    "        return vmap(partial(self.model_apply_fun, params))(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = 50\n",
    "model_vanilla_lstm = VanillaLSTM(\n",
    "    one_hot_encoded_shape = (padding_length, 21)\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "model_vanilla_lstm.fit(lstm_train_oh, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: GRU\n",
    "### Implementation of GRU from https://towardsdatascience.com/getting-started-with-jax-mlps-cnns-rnns-d0bc389bd683\n",
    "##### formula of GRU can be found at https://www.google.com/search?q=GRU+formula&tbm=isch&source=iu&ictx=1&fir=VgQCUBNNXFcaTM%252CXHoefSnHEDF68M%252C_&vet=1&usg=AI4_-kSHwsjZClZeI1h2s23kwDj5ncW6Jg&sa=X&ved=2ahUKEwjcwZaW8vjvAhUHKFkFHc8NC8sQ9QF6BAgNEAE&biw=1680&bih=895#imgrc=VgQCUBNNXFcaTM\n",
    "##### machanism of GRU can be found at https://d2l.ai/chapter_recurrent-modern/gru.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.nn import sigmoid\n",
    "from jax.nn.initializers import glorot_normal, normal\n",
    "\n",
    "from functools import partial\n",
    "from jax import lax\n",
    "\n",
    "def GRU(out_dim, W_init=glorot_normal(), b_init=normal()):\n",
    "    def init_fun(rng, input_shape):\n",
    "        \"\"\" Initialize the GRU layer for stax \"\"\"\n",
    "        # input shape is of (batch_size, num_time_steps, embeddings)\n",
    "        hidden = b_init(rng, (input_shape[0], out_dim))\n",
    "\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        update_W, update_U, update_b = (\n",
    "            W_init(k1, (input_shape[2], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),)\n",
    "\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        reset_W, reset_U, reset_b = (\n",
    "            W_init(k1, (input_shape[2], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),)\n",
    "\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        out_W, out_U, out_b = (\n",
    "            W_init(k1, (input_shape[2], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),)\n",
    "        # Input dim 0 represents the batch dimension\n",
    "        # Input dim 1 represents the time dimension (before scan moveaxis)\n",
    "        output_shape = (input_shape[0], input_shape[1], out_dim)\n",
    "        return (output_shape,\n",
    "            (hidden,\n",
    "             (update_W, update_U, update_b),\n",
    "             (reset_W, reset_U, reset_b),\n",
    "             (out_W, out_U, out_b),),)\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        \"\"\" Loop over the time steps of the input sequence \"\"\"\n",
    "        h = params[0]\n",
    "        \n",
    "        def apply_fun_scan(params, hidden, inp):\n",
    "            \"\"\" Perform single step update of the network \"\"\"\n",
    "            _, (update_W, update_U, update_b), (reset_W, reset_U, reset_b), (\n",
    "                out_W, out_U, out_b) = params\n",
    "\n",
    "            update_gate = sigmoid(np.dot(inp, update_W) +\n",
    "                                  np.dot(hidden, update_U) + update_b)\n",
    "            reset_gate = sigmoid(np.dot(inp, reset_W) +\n",
    "                                 np.dot(hidden, reset_U) + reset_b)\n",
    "            output_gate = np.tanh(np.dot(inp, out_W) \n",
    "                                  + np.dot(np.multiply(reset_gate, hidden), out_U) \n",
    "                                  + out_b)\n",
    "            output = np.multiply(update_gate, hidden) + np.multiply(1-update_gate, output_gate)\n",
    "            hidden = output\n",
    "            return hidden, hidden\n",
    "\n",
    "        # Move the time dimension to position 0\n",
    "        inputs = np.moveaxis(inputs, 1, 0)\n",
    "        f = partial(apply_fun_scan, params)\n",
    "        _, h_new = lax.scan(f, h, inputs)\n",
    "        return h_new\n",
    "\n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = 10              # Number of OU timesteps\n",
    "batch_size = 64            # Batchsize\n",
    "num_hidden_units = 12      # GRU cells in the RNN layer \n",
    "\n",
    "# Initialize the network and perform a forward pass\n",
    "init_fun, gru_rnn = stax.serial(Dense(num_hidden_units), Relu,\n",
    "                                GRU(num_hidden_units), Dense(1))\n",
    "_, params = init_fun(key, (batch_size, num_dims, 1))\n",
    "\n",
    "def mse_loss(params, inputs, targets):\n",
    "    \"\"\" Calculate the Mean Squared Error Prediction Loss. \"\"\"\n",
    "    preds = gru_rnn(params, inputs)\n",
    "    return np.mean((preds - targets)**2)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y, opt_state):\n",
    "    \"\"\" Perform a forward pass, calculate the MSE & perform a SGD step. \"\"\"\n",
    "    loss, grads = value_and_grad(mse_loss)(params, x, y)\n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return get_params(opt_state), opt_state, loss\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patch-gnn",
   "language": "python",
   "name": "patch-gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
