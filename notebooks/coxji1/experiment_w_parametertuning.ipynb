{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experiment with parameter tuning \n",
    "Using [Optuna](https://optuna.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_gnn.data import load_ghesquire\n",
    "import pandas as pd\n",
    "from pyprojroot import here\n",
    "import pickle as pkl\n",
    "from patch_gnn.splitting import train_test_split\n",
    "from jax import random\n",
    "from patch_gnn.seqops import one_hot\n",
    "from patch_gnn.unirep import unirep_reps\n",
    "from patch_gnn.graph import graph_tensors\n",
    "from patch_gnn.models import MPNN, DeepMPNN, DeepGAT\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import explained_variance_score as evs\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle as pkl\n",
    "from patch_gnn.graph import met_position\n",
    "import seaborn as sns\n",
    "from jax.config import config\n",
    "import jax.numpy as np\n",
    "import optuna\n",
    "from typing import Dict\n",
    "config.update(\"jax_debug_nans\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate data for MPNN model and Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(490)\n",
    "k1,k2,k3,k4 = random.split(key, num = 4)\n",
    "n_node = 5\n",
    "n_feature =13\n",
    "node_feature_shape = (n_node, n_feature) \n",
    "n_adjacency = 1\n",
    "num_training = 5\n",
    "\n",
    "adjacency = random.normal(k1, (251,n_node,n_node,n_adjacency))\n",
    "node_features = random.normal(k2, (251, n_node, n_feature))\n",
    "X = (adjacency, node_features)\n",
    "X_rf = random.normal(k4, (251, 630))\n",
    "y = random.normal(k3, (251, 1))\n",
    "mpnn_model = MPNN(node_feature_shape=node_feature_shape,\n",
    "                                num_adjacency = n_adjacency,\n",
    "                                num_training_steps =num_training, \n",
    "                                optimizer_step_size = 1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first test evotune on mpnn class models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnn_model = MPNN(node_feature_shape=node_feature_shape,\n",
    "                                num_adjacency = n_adjacency,\n",
    "                                num_training_steps =num_training, \n",
    "                                optimizer_step_size = 1e-5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evotune_mpnn_class(model, X, y, num_training_steps_kwargs:Dict = {}, optimizer_step_size_kwargs:Dict={}, n_trials:int = 10 ):\n",
    "    \"\"\"\n",
    "    evotune for MPNN class models\n",
    "    One can choose to tune num_training_steps (num of epochs) and optimizer_step_size (learning rate)\n",
    "    \n",
    "    :param X: input data for MPNN class models\n",
    "    :param y: outcome associated with input data, should be of shape (sample_size,), use np.squeeze(y) if not\n",
    "    :params num_training_steps_kwargs: a dictionary with the format of, \n",
    "                                num_training_steps_kwargs = {\n",
    "                                \"name\": \"num_training_steps\", #requires to have as is\n",
    "                                \"low\": 10, # one can change it\n",
    "                                \"high\": 14,  # one can change it\n",
    "                                \"log\" :True # one can change it\n",
    "                            }, default is an empty dictionary with means no hyperparameters should be tuned\n",
    "                            \n",
    "    :params optimizer_step_size_kwargs: a dictionary with the format of, \n",
    "                                optimizer_step_size_kwargs = {\n",
    "                                \"name\" : \"optimizer_step_size\",# this key value pair is required as is\n",
    "                                \"low\" : 1e-5, # one can change the value\n",
    "                                \"high\" : 1e-2, # one can change the value\n",
    "                            } \n",
    "    :params n_trials: number of experiments for optuna to run, each experiment is associated with one hyperparameter combination\n",
    "                            \n",
    "    return:\n",
    "            The ideal param combination (that one asked to tune) with the lowest MSE error on input data in \n",
    "            in given number of experiments\n",
    "    \"\"\"\n",
    "    if len(num_training_steps_kwargs)==0 and len(optimizer_step_size_kwargs) ==0:\n",
    "        raise ValueError(\"The hyperparameters to optimize cannot be empty\")\n",
    "    def objective(trial):\n",
    "        param_dict = {}\n",
    "        #defensive programming to check for empty values\n",
    "        if len(num_training_steps_kwargs)!=0:\n",
    "            num_training_steps = trial.suggest_int(**num_training_steps_kwargs)\n",
    "            #update param_dict\n",
    "            param_dict[\"num_training_steps\"] = num_training_steps\n",
    "        if len(optimizer_step_size_kwargs)!=0:\n",
    "            optimizer_step_size = trial.suggest_uniform(**optimizer_step_size_kwargs)\n",
    "            param_dict[\"optimizer_step_size\"] = optimizer_step_size\n",
    "        \n",
    "        print(f\"The params that were optimized is {param_dict}\")\n",
    "        # the model object is callable and takes a dict argument to update the parameters\n",
    "        mpnn_obj = model(param_dict = param_dict)\n",
    "\n",
    "        loss_history = mpnn_obj.fit(X, y).loss_history\n",
    "        print(f\"num_training_step is {mpnn_obj.num_training_steps}\")\n",
    "        loss = loss_history[mpnn_obj.num_training_steps-1]\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:42:45,624]\u001b[0m A new study created in memory with name: no-name-64b67250-b70b-4760-97bb-e731cf5f0529\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The params that were optimized is {'num_training_steps': 229}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4645f090ee7c44e5b823cd038c434099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:42:53,060]\u001b[0m Trial 0 finished with value: 0.9240721464157104 and parameters: {'num_training_steps': 229}. Best is trial 0 with value: 0.9240721464157104.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_step is 229\n",
      "The params that were optimized is {'num_training_steps': 89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12d18765b494306a046581079063733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:42:56,151]\u001b[0m Trial 1 finished with value: 1.0165961980819702 and parameters: {'num_training_steps': 89}. Best is trial 0 with value: 0.9240721464157104.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_step is 89\n",
      "The params that were optimized is {'num_training_steps': 110}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47381860fdbb4b30b9ffc626b96de601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:42:59,689]\u001b[0m Trial 2 finished with value: 1.5875941514968872 and parameters: {'num_training_steps': 110}. Best is trial 0 with value: 0.9240721464157104.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_step is 110\n",
      "The params that were optimized is {'num_training_steps': 8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431ea065816b4d6cb50851aed7e1d5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:43:00,529]\u001b[0m Trial 3 finished with value: 1.4208685159683228 and parameters: {'num_training_steps': 8}. Best is trial 0 with value: 0.9240721464157104.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_step is 8\n",
      "The params that were optimized is {'num_training_steps': 6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743d069f87c849af812d167d99b4b9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:43:01,544]\u001b[0m Trial 4 finished with value: 1.4959734678268433 and parameters: {'num_training_steps': 6}. Best is trial 0 with value: 0.9240721464157104.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_step is 6\n",
      "The params that were optimized is {'num_training_steps': 348}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ea7f730a34404e878c5006f51781e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/348 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:43:13,087]\u001b[0m Trial 5 finished with value: 0.91261225938797 and parameters: {'num_training_steps': 348}. Best is trial 5 with value: 0.91261225938797.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_step is 348\n",
      "The params that were optimized is {'num_training_steps': 125}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28bc56e0087433eb70ab5229a7a1169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:43:17,607]\u001b[0m Trial 6 finished with value: 1.582926630973816 and parameters: {'num_training_steps': 125}. Best is trial 5 with value: 0.91261225938797.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_step is 125\n",
      "The params that were optimized is {'num_training_steps': 15}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af34e1accd774cd8b213cb4f12af0579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:43:18,646]\u001b[0m Trial 7 finished with value: 1.6999303102493286 and parameters: {'num_training_steps': 15}. Best is trial 5 with value: 0.91261225938797.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_step is 15\n",
      "The params that were optimized is {'num_training_steps': 161}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf38493a4f0c4f2bbf542b460ba3e80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:43:24,243]\u001b[0m Trial 8 finished with value: 1.4698224067687988 and parameters: {'num_training_steps': 161}. Best is trial 5 with value: 0.91261225938797.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_step is 161\n",
      "The params that were optimized is {'num_training_steps': 32}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1686fa8111ff41a79e48901d60bb7723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:43:25,794]\u001b[0m Trial 9 finished with value: 2.4718165397644043 and parameters: {'num_training_steps': 32}. Best is trial 5 with value: 0.91261225938797.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_step is 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_training_steps': 348}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_training_steps_kwargs = {\n",
    "    \"name\": \"num_training_steps\",\n",
    "    \"low\": 4,\n",
    "    \"high\": 500,\n",
    "    \"log\": True,\n",
    "}\n",
    "#num_training_steps_kwargs={}\n",
    "optimizer_step_size_kwargs={\n",
    "    \"name\" : \"optimizer_step_size\",# this key value pair is required as is\n",
    "    \"low\" : 1e-5, # one can change the value\n",
    "    \"high\" : 1e-2, # one can change the value\n",
    "} \n",
    "optimizer_step_size_kwargs={}\n",
    "evotune_mpnn_class(mpnn_model, X, y, num_training_steps_kwargs,optimizer_step_size_kwargs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### then test evotune on random forest class models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_max_depth_kwargs = {\n",
    "    \"name\" : \"rf_max_depth\",\n",
    "    \"low\" :2, \n",
    "    \"high\" : 32,\n",
    "    \"log\" :True\n",
    "}\n",
    "rf_n_estimator_kwargs = {\n",
    "    \"name\" : \"n_estimators\",\n",
    "    \"low\" : 64,\n",
    "    \"high\" : 128,\n",
    "    \"log\" : True\n",
    "} \n",
    "\n",
    "def evotune_rf( X, y, rf_max_depth_kwargs: Dict ={}, rf_n_estimator_kwargs: Dict = {} ,n_trials:int = 20):\n",
    "    \"\"\"\n",
    "    evotune for random forest models\n",
    "    One can choose to tune max_depth and n_estimators (num of trees in rf)\n",
    "    \n",
    "    :param X: input data for random forest models\n",
    "    :param y: outcome associated with input data, should be of shape (sample_size,), use np.squeeze(y) if not\n",
    "    :params rf_max_depth_kwargs: a dictionary with the format of, \n",
    "                                rf_max_depth_kwargs = {\n",
    "                                \"name\" : \"rf_max_depth\", # this key value pair is required as is\n",
    "                                \"low\" :2,  # one can change the value\n",
    "                                \"high\" : 32, # one can change the value\n",
    "                                \"log\" :True  #this key value pair is required as is\n",
    "                            }, default is an empty dictionary with means no hyperparameters should be tuned\n",
    "                            \n",
    "    :params rf_n_estimator_kwargs: a dictionary with the format of, \n",
    "                                rf_n_estimator_kwargs = {\n",
    "                                \"name\" : \"n_estimators\",# this key value pair is required as is\n",
    "                                \"low\" : 64, # one can change the value\n",
    "                                \"high\" : 128, # one can change the value\n",
    "                                \"log\" : True # this key value pair is required as is\n",
    "                            } \n",
    "                            \n",
    "    :params n_trials: number of experiments for optuna to run, each experiment is associated with one hyperparameter combination\n",
    "                            \n",
    "    return:\n",
    "            The ideal param combination (that one asked to tune) with the lowest MSE error on input data in \n",
    "            in given number of experiments\n",
    "\n",
    "    \"\"\"\n",
    "    if len(rf_max_depth_kwargs)==0 and len(rf_n_estimator_kwargs) ==0:\n",
    "        raise ValueError(\"The hyperparameters to optimize cannot be empty\")\n",
    "    def objective(trial):\n",
    "        param_dict = {}\n",
    "        #defensive programming to check for empty values\n",
    "        if len(rf_max_depth_kwargs)!=0:\n",
    "            max_depth = trial.suggest_int(**rf_max_depth_kwargs)\n",
    "            #update param_dict\n",
    "            param_dict[\"max_depth\"] = max_depth\n",
    "        if len(rf_n_estimator_kwargs)!=0:\n",
    "            n_estimators = trial.suggest_int(**rf_n_estimator_kwargs)\n",
    "            param_dict[\"n_estimators\"] = n_estimators\n",
    "        \n",
    "        print(f\"The params that were optimized {param_dict}\")\n",
    "        # build rf model object\n",
    "        rf_obj = RandomForestRegressor(oob_score=True, n_jobs=-1, **param_dict)\n",
    "        #rf_obj = model(**param_dict) # this won't work since this class doens't have __call__\n",
    "\n",
    "        rf_obj.fit(X, y)\n",
    "        y_pred = rf_obj.predict(X)\n",
    "\n",
    "        error = mean_squared_error(y, y_pred)\n",
    "\n",
    "        return error\n",
    "\n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with random forest tuning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:14:41,073]\u001b[0m A new study created in memory with name: no-name-c68cd51d-0886-47d1-b9b0-0dec90ecf432\u001b[0m\n",
      "\u001b[32m[I 2021-07-01 20:14:41,253]\u001b[0m Trial 0 finished with value: 0.5664913199420887 and parameters: {'rf_max_depth': 3, 'n_estimators': 87}. Best is trial 0 with value: 0.5664913199420887.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The params that were optimized {'max_depth': 3, 'n_estimators': 87}\n",
      "The params that were optimized {'max_depth': 21, 'n_estimators': 110}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:14:41,525]\u001b[0m Trial 1 finished with value: 0.13180692704174113 and parameters: {'rf_max_depth': 21, 'n_estimators': 110}. Best is trial 1 with value: 0.13180692704174113.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The params that were optimized {'max_depth': 6, 'n_estimators': 126}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:14:42,085]\u001b[0m Trial 2 finished with value: 0.23002004743304078 and parameters: {'rf_max_depth': 6, 'n_estimators': 126}. Best is trial 1 with value: 0.13180692704174113.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The params that were optimized {'max_depth': 15, 'n_estimators': 93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-01 20:14:42,344]\u001b[0m Trial 3 finished with value: 0.13942848849655806 and parameters: {'rf_max_depth': 15, 'n_estimators': 93}. Best is trial 1 with value: 0.13180692704174113.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rf_max_depth': 21, 'n_estimators': 110}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evotune_rf( X_rf, np.squeeze(y), rf_max_depth_kwargs, rf_n_estimator_kwargs, n_trials =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patch-gnn",
   "language": "python",
   "name": "patch-gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
