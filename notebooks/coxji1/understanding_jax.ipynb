{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import lax, nn, random, vmap\n",
    "from jax._src.nn.functions import normalize\n",
    "from jax.experimental import stax\n",
    "from jax.nn.initializers import glorot_normal\n",
    "from jax.random import normal\n",
    "from functools import partial\n",
    "from patch_gnn.layers import softmax_on_non_zero,concatenate_node_features,get_norm_attn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(12)\n",
    "k1, k2,k3,k4,k5 = random.split(key, num = 5)\n",
    "n_features =5\n",
    "n_output_dims =13\n",
    "n_node = 17\n",
    "w = random.normal(k1, (n_features, n_output_dims))\n",
    "a = random.normal(k2, (n_output_dims * 2,))\n",
    "nfa = random.normal(k3, (n_features,))\n",
    "adjacency_matrix = random.normal(k4, (n_node,n_node))\n",
    "node_embeddings = random.normal(k5, (n_node, n_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 17)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_feat_attn = vmap(partial(np.multiply, np.abs(nfa)))(node_embeddings)\n",
    "node_projection = np.dot(\n",
    "        node_feat_attn, w\n",
    "    )\n",
    "node_by_node_concat = concatenate_node_features(\n",
    "        node_projection\n",
    "    )\n",
    "projection = np.dot(node_by_node_concat, a)\n",
    "atten_leaky_relu = nn.leaky_relu(projection, negative_slope=0.1)\n",
    "atten_leaky_relu = np.squeeze(atten_leaky_relu)\n",
    "attention = atten_leaky_relu * np.squeeze(\n",
    "        adjacency_matrix\n",
    "    )\n",
    "norm_attention = softmax_on_non_zero(attention, adjacency_matrix)#vmap(partial(softmax_on_non_zero, adj =adjacency_matrix ))(attention)\n",
    "norm_attention.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mp.shape is (5, 13, 7)\n",
      "np.dot(mp, adjacency_weights) shape is (5, 13, 1)\n",
      "np.squeeze(np.dot(mp, adjacency_weights)) shape is (5, 13, 1)\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(12)\n",
    "k1, k2 = random.split(key)\n",
    "n_patches = 3\n",
    "n_node = 5\n",
    "n_features = 13\n",
    "n_adjacencies = 7\n",
    "\n",
    "A = random.normal(k1, (n_node,n_node, n_adjacencies))  # adjacency matrix, 5 node * 5 node\n",
    "F = random.normal(k2, (n_node, n_features)) # feature matrix, 5 node *13 features\n",
    "adjacency_weights_init=glorot_normal() # init weight \n",
    "adjacency_weights = adjacency_weights_init(k1, (n_adjacencies, 1))\n",
    "adjacency_weights.shape\n",
    "mp = vmap(np.dot, in_axes=(-1, None), out_axes=(-1))(A, F)\n",
    "\n",
    "print(f\"mp.shape is {mp.shape}\") #shape (n_nodes, n_features, n_adjacency_like_matrics) \n",
    "print(f\"np.dot(mp, adjacency_weights) shape is {np.dot(mp, adjacency_weights).shape}\")\n",
    "print(f\"np.squeeze(np.dot(mp, adjacency_weights)) shape is {(np.dot(mp, adjacency_weights)).shape}\") # shape (n_nodes, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_gnn.layers import MessagePassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fun, apply_fun = MessagePassing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(n_nodes, n_features), (adjacency_weights)  = init_fun(random.PRNGKey(12), input_shape = (5,13,1))\n",
    "adjacency_weights.shape # (1,1)\n",
    "out = apply_fun(adjacency_weights[0][0], (A, F)) # why is message passing params not a matrix?\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_adjacencies = 1\n",
    "adjacency_weights = random.normal(k1, (n_adjacencies, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_out = np.squeeze(np.dot(mp, params))\n",
    "mp_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_gnn.layers import concatenate_node_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### according to the current implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However, alternatively if node_by_node_cat and a are different shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "node_by_node_concat = random.normal(k1, (n_node, 2*n_output_dims))\n",
    "a = random.normal(k2, (n_node, 2*n_output_dims))\n",
    "\n",
    "projection = np.dot(node_by_node_concat, a.T)\n",
    "print(projection.shape)\n",
    "atten_after_relu = nn.leaky_relu(projection, negative_slope=0.1)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 3.1671557 ,  7.295389  ,  1.4041579 ],\n",
       "             [-1.2738957 , -0.39845598, -0.28209615],\n",
       "             [ 5.189487  , -0.09320077, -0.586724  ]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_after_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 3.1671557 ,  0.        ,  0.        ],\n",
       "             [-1.2738957 , -0.39845598,  0.        ],\n",
       "             [ 5.189487  , -0.09320077, -0.586724  ]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tril(atten_after_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_on_non_zero(vect):\n",
    "    \"\"\"\n",
    "    Apply softmax normalization on a vector and ignore the 0 values\n",
    "    For example: [-0.3, 0 , 0] denotes the attention of a node on three \n",
    "    nodes, this node has no value on node 2 or 3, and only -0.3 value on\n",
    "    itself, the softmax will return [1,0,0], given \"0\" doesn't participate in\n",
    "    softmax calculation\n",
    "    \n",
    "    :param vect: a column vector from a matrix output from `leaky_relu`\n",
    "    :return a list with softmax normalized values\n",
    "    \"\"\"\n",
    "    vect_non_zero = np.asarray([i for i in vect if i!=0])\n",
    "    return nn.softmax(vect_non_zero).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_attn_matrix(atten_after_relu):\n",
    "    \"\"\"\n",
    "    Apply softmax normalization on a matrix of shape (n_node, n_node).\n",
    "    Turn upper triangle of this matrix to value of 0s to represent that \n",
    "    the attention of a given node only applies to the nodes before the \n",
    "    current node\n",
    "    \n",
    "    :param output: the attention matrix coming out from `leaky_relu`\n",
    "    \n",
    "    :returns matrix: a numpy.ndarray triangle matrix of shape (n_node, n_node)\n",
    "    denoting how much each node pays attention to previous nodes. The attention\n",
    "    of each node on other node should sum up to 1\n",
    "\n",
    "    \"\"\"\n",
    "    matrix = []\n",
    "    # turn previous atten matrix to triagnle matrix\n",
    "    # btw, when performing masking, the matrix coming out from \n",
    "    # leaky_relu is not symmetrical, just always ignore the upper triangle of the matrix?\n",
    "    atten_after_relu = np.tril(atten_after_relu)\n",
    "    for i in range(len(atten_after_relu)):\n",
    "        matrix.append(softmax_on_non_zero(atten_after_relu[i,:]))\n",
    "    pad = len(max(matrix, key =len))\n",
    "    matrix  = numpy.array([i + [0]*(pad-len(i)) for i in matrix])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0]\n",
      "[0.29412367939949036, 0.705876350402832]\n",
      "[0.9918871521949768, 0.005037559196352959, 0.0030752879101783037]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(output)):\n",
    "    print(softmax_on_non_zero(output[i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(get_norm_attn_matrix(atten_after_relu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        ],\n",
       "       [0.29412368, 0.70587635, 0.        ],\n",
       "       [0.99188715, 0.00503756, 0.00307529]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_norm_attn_matrix(atten_after_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., nan, nan],\n",
       "       [ 1.,  1., nan],\n",
       "       [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import numpy\n",
    "#a=  numpy.empty([3,3])\n",
    "#a[:] = 1\n",
    "#a[numpy.tril_indices(a.shape[0], -1)] = numpy.nan\n",
    "#a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test message passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_gnn.layers import MessagePassing\n",
    "from jax.experimental import stax\n",
    "from jax.random import normal,PRNGKey\n",
    "from jax import random,jit\n",
    "from jax.experimental.optimizers import adam\n",
    "from functools import partial\n",
    "from patch_gnn.training import mseloss, step\n",
    "from tqdm.auto import tqdm\n",
    "import jax.numpy as np\n",
    "from patch_gnn.models import MPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjs = random.normal(k1, (3,20,20, 1))\n",
    "feats = random.normal(k1, (3,20,67))\n",
    "train_graph = (adjs, feats)\n",
    "train_target = random.normal(k2, (3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76d948689b4478cb4caee172431fb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mp is Traced<ShapedArray(float32[20,67,1])>with<BatchTrace(level=3/1)>\n",
      "  with val = Traced<ShapedArray(float32[3,20,67,1])>with<DynamicJaxprTrace(level=0/1)>\n",
      "       batch_dim = 0\n",
      " output is Traced<ShapedArray(float32[20,67])>with<BatchTrace(level=3/1)>\n",
      "  with val = Traced<ShapedArray(float32[3,20,67])>with<JVPTrace(level=2/1)>\n",
      "               with primal = Traced<ShapedArray(float32[3,20,67])>with<DynamicJaxprTrace(level=0/1)>\n",
      "                    tangent = Traced<ShapedArray(float32[3,20,67]):JaxprTrace(level=1/1)>\n",
      "       batch_dim = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<patch_gnn.models.MPNN at 0x2aacfff90430>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_training_steps = 10\n",
    "node_feature_shape = (20,67)\n",
    "num_adjacency =1\n",
    "model_mpnn = MPNN(\n",
    "        node_feature_shape=node_feature_shape,\n",
    "        num_adjacency=num_adjacency,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "model_mpnn.fit(train_graph, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DeviceArray(95.95095, dtype=float32),\n",
       " DeviceArray(10.907946, dtype=float32),\n",
       " DeviceArray(29.840017, dtype=float32),\n",
       " DeviceArray(52.032665, dtype=float32),\n",
       " DeviceArray(38.774757, dtype=float32),\n",
       " DeviceArray(16.14018, dtype=float32),\n",
       " DeviceArray(6.3733273, dtype=float32),\n",
       " DeviceArray(12.946376, dtype=float32),\n",
       " DeviceArray(23.326376, dtype=float32),\n",
       " DeviceArray(25.031109, dtype=float32)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mpnn.loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patch-gnn",
   "language": "python",
   "name": "patch-gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
