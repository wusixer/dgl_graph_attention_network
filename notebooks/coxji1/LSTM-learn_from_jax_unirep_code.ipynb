{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax.numpy as np\n",
    "from jax import lax, nn, random, vmap\n",
    "from jax._src.nn.functions import normalize\n",
    "from jax.experimental import stax\n",
    "from jax.nn import sigmoid\n",
    "from jax.nn.initializers import glorot_normal, normal\n",
    "from jax.random import normal as norm\n",
    "from jax import lax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from patch_gnn.data import load_ghesquire\n",
    "import pandas as pd\n",
    "from pyprojroot import here\n",
    "import pickle as pkl\n",
    "from patch_gnn.splitting import train_test_split\n",
    "from jax import random\n",
    "from patch_gnn.seqops import one_hot\n",
    "from patch_gnn.unirep import unirep_reps\n",
    "from patch_gnn.graph import graph_tensors\n",
    "from patch_gnn.models import MPNN, DeepMPNN\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import explained_variance_score as evs\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pickle as pkl\n",
    "from patch_gnn.graph import met_position\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset, split into train and test, then one hot encode it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of train_df and test_df are (258, 18), (111, 18)\n",
      "the shape of train_oh and test_oh are (258, 1050), (111, 1050)\n"
     ]
    }
   ],
   "source": [
    "# load graphs\n",
    "graph_pickle_path = here() / \"data/ghesquire_2011/graphs.pkl\"\n",
    "with open(graph_pickle_path, \"rb\") as f:\n",
    "    graphs = pkl.load(f)\n",
    "\n",
    "# load data\n",
    "data = load_ghesquire()\n",
    "\n",
    "# filter data based on graphs.keys() and other metrics\n",
    "filtered = (\n",
    "   data\n",
    "   .query(\"`accession-sequence` in @graphs.keys()\")\n",
    "   .query(\"ox_fwd_logit < 2.0\")\n",
    "   .join_apply(met_position, \"met_position\")\n",
    ")\n",
    "\n",
    "# split data into train and testing\n",
    "key = random.PRNGKey(490)\n",
    "train_df, test_df = train_test_split(key, filtered) # 70% training, 30% testing\n",
    "print(f\"the shape of train_df and test_df are {train_df.shape}, {test_df.shape}\")\n",
    "\n",
    "# pad sequence to 50 length and one hot encode it\n",
    "padding_length = 50\n",
    "train_oh = one_hot(train_df, padding_length) \n",
    "test_oh = one_hot(test_df,padding_length)\n",
    "print(f\"the shape of train_oh and test_oh are {train_oh.shape}, {test_oh.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reshape data to fit LSTM and get target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(258, 50, 21) (111, 50, 21)\n"
     ]
    }
   ],
   "source": [
    "lstm_train_oh = train_oh.reshape(train_oh.shape[0], padding_length, 21)\n",
    "lstm_test_oh = test_oh.reshape(test_oh.shape[0], padding_length, 21)\n",
    "print(lstm_train_oh.shape, lstm_test_oh.shape)\n",
    "train_target = train_df['ox_fwd_logit'].values\n",
    "test_target = test_df['ox_fwd_logit'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 50, 21), (1,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "lstm_train_oh[0:batch_size,:,:].shape, train_target[0:batch_size,].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM based on JAX unirep's case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AAEmbedding(embedding_dims: int , E_init=glorot_normal(), **kwargs):\n",
    "    \"\"\"\n",
    "    Initial n-dimensional embedding of each amino-acid, like a dense layer without bias\n",
    "    \"\"\"\n",
    "\n",
    "    def init_fun(rng, input_shape):\n",
    "        \"\"\"\n",
    "        Generates the inital AA embedding matrix.\n",
    "        `input_shape`:\n",
    "            one-hot encoded AA sequence -> (n_aa, n_unique_aa)\n",
    "        `output_dims`:\n",
    "            embedded sequence -> (n_aa, embedding_dims)\n",
    "        `emb_matrix`:\n",
    "            embedding matrix -> (n_unique_aa, embedding_dims)\n",
    "        \"\"\"\n",
    "        emb_matrix = E_init(rng, (input_shape[-1], embedding_dims))\n",
    "        output_dims = (-1, embedding_dims)#-1 means it can be of any number \n",
    "    \n",
    "        return output_dims, emb_matrix\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Embed a single AA sequence\n",
    "        \"\"\"\n",
    "        emb_matrix = params\n",
    "        # (n_aa, n_unique_aa) * (n_unique_aa, embedding_dims) => (n_aa, embedding_dims) # noqa: E501\n",
    "        return np.matmul(inputs, emb_matrix)\n",
    "\n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(out_dim, W_init=glorot_normal(), b_init=normal()):\n",
    "    \"\"\"\n",
    "    one directional LSTM, see math here https://d2l.ai/chapter_recurrent-modern/lstm.html\n",
    "    :params out_dim: number of output neurons associated with an input of a single time point\n",
    "    \n",
    "    \"\"\"\n",
    "    def init_fun(rng, input_shape):\n",
    "        \"\"\"\n",
    "        initialize LSTM layer for stax\n",
    "        :param rng: The PRNGKey (from JAX) for random number generation _reproducibility_.\n",
    "        :params input_shape: (num_time_steps/n_letters, embeddings)\n",
    "        \"\"\"\n",
    "        # initial hidden state and memory state ---- don't set it this way since params are not supposed to be tuple of tuples\n",
    "        #hidden = b_init(rng, (1, out_dim)) # denote by H in the formula #b_init(rng, (input_shape[0], out_dim)), instead None can be any number\n",
    "        #memory = b_init(rng, (1, out_dim)) # denote by C in the formula #b_init(rng, (input_shape[0], out_dim))\n",
    "        # input gate\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        input_W, input_U, input_b = (\n",
    "            W_init(k1, (input_shape[-1], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),\n",
    "        )\n",
    "        # forget gate\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        forget_W, forget_U, forget_b = (\n",
    "            W_init(k1, (input_shape[-1], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),\n",
    "        )\n",
    "        # output gate\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        output_W, output_U, output_b = (\n",
    "            W_init(k1, (input_shape[-1], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),\n",
    "        )\n",
    "        # current memory \n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        candidate_m_W, candidate_m_U, candidate_m_b = (\n",
    "            W_init(k1, (input_shape[-1], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        output_shape = (1,out_dim) #(out_dim, )#,input_shape[0] or 1? I think it should be 1 since each sample is input into the algorithm each time. input_shape[0] should be dim of the previous layer\n",
    "        return (output_shape, (input_W, input_U, input_b, forget_W, forget_U, forget_b, output_W, output_U, output_b, candidate_m_W, candidate_m_U, candidate_m_b))# this tuple is (output_shape, params)\n",
    "        #return (output_shape,\n",
    "        #       ((hidden, memory),\n",
    "        #       (input_W, input_U, input_b),\n",
    "        #       (forget_W, forget_U, forget_b),\n",
    "        #       (output_W, output_U, output_b),\n",
    "        #       (candidate_m_W, candidate_m_U, candidate_m_b)),) # this tuple is (output_shape, params)\n",
    "    \n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        \"\"\" Loop over the time steps of the input sequence \"\"\"\n",
    "        #h_0, m_0 = params[0] # initial hidden and memory, not do it this way since params contain values one is going to optimize\n",
    "        h_0 = np.zeros((1, out_dim))\n",
    "        m_0 = np.zeros((1, out_dim))\n",
    "        #print(f\"inputs shape {inputs.shape}\")  # input should be (num_timestep, embedding), num_timestep=50\n",
    "\n",
    "        def apply_fun_scan(params, carry, inputs):\n",
    "            \"\"\" \n",
    "            Perform single step update of the network\n",
    "            carry: a tuple with (hidden, memory)\n",
    "            :param input: of shape \n",
    "            \"\"\"\n",
    "            (hidden, memory) = carry  # carry has to be a tuple, b/c this function will be used by lax.scan and it has to be 2 inputs not 3\n",
    "            input_W, input_U, input_b, forget_W, forget_U, forget_b, output_W, output_U, output_b, candidate_m_W, candidate_m_U, candidate_m_b = params\n",
    "#             (i,j), (input_W, input_U, input_b), (forget_W, forget_U, forget_b), (\n",
    "#                 output_W, output_U, output_b),(\n",
    "#                 candidate_m_W, candidate_m_U, candidate_m_b) = params\n",
    "            #print(f\"input_W shape: {input_W.shape}, input_U shape: {input_U.shape}, input_b shape: {input_b.shape}\" )\n",
    "            # shape annotation: (1,embedding)*(embedding, outdim) +(1, outdim)*(outdim, outdim) => (1, outdim)\n",
    "            input_gate = sigmoid(np.dot(inputs, input_W) +\n",
    "                                  np.dot(hidden, input_U) + input_b)\n",
    "            # shape annotation: (1,embedding)*(embedding, outdim) +(1, outdim)*(outdim, outdim)=> (1, outdim)\n",
    "            forget_gate = sigmoid(np.dot(inputs, forget_W) +\n",
    "                                 np.dot(hidden, forget_U) + forget_b)\n",
    "            # shape annotation: (1,embedding)*(embedding, outdim) +(1, outdim)*(outdim, outdim)=> (1, outdim)\n",
    "            output_gate = sigmoid(np.dot(inputs, output_W) +\n",
    "                                 np.dot(hidden, output_U) + output_b)\n",
    "            # shape annotation: (1,embedding)*(embedding, outdim) +(1, outdim)*(outdim, outdim)=> (1, outdim)\n",
    "            candidate_memory = np.tanh(np.dot(inputs, candidate_m_W) +\n",
    "                                 np.dot(hidden, candidate_m_U) + candidate_m_b)\n",
    "            # shape annotation: (1,outdim)@(1,outdim) + 1,outdim)@(1,outdim)=> (1, outdim)\n",
    "            #print(f\"forget_gate shape: {forget_gate.shape}, memory shape: {memory.shape}, input_gate shape:{input_gate.shape}, cadidate_memory shape: {candidate_memory.shape}\")\n",
    "            current_memory = np.multiply(forget_gate, memory) + np.multiply(input_gate, candidate_memory)\n",
    "            # shape annotation: (1,outdim)@(1, outdim)=> (1, outdim)\n",
    "            current_hidden = np.multiply(output_gate, np.tanh(current_memory))\n",
    "            #print(f\"current_memory shape: {current_memory.shape}, hidden shape: {hidden.shape}\")\n",
    "            hidden = current_hidden\n",
    "            memory = current_memory\n",
    "\n",
    "            return (hidden, memory), hidden  # it has to be in this format because lax.scan output requires it to be of the shape of the function output\n",
    "\n",
    "        f = partial(apply_fun_scan, params) # this is a function\n",
    "        (h_final, m_final), output = lax.scan(f, init=(h_0, m_0), xs=inputs)\n",
    "        \n",
    "        #print(f\"h_final shape is {h_final.shape}, m_final shape is {m_final.shape}, output shape is {output.shape}\")\n",
    "        return (h_final, m_final), output #h_final shape is (1, outdim), m_final shape is (1, outdim), output shape is (n_timestep, 1, outdim)\n",
    "\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to make another layer between lstm to dense to get rid of the tuple returned by lstm\n",
    "def LSTMHiddenStates(**kwargs):\n",
    "    \"\"\"\n",
    "    Returns the full hidden states (last element) of the mLSTM layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def init_fun(rng, input_shape):\n",
    "        output_shape = (input_shape[1],) # input_shape is the output shape of the LSTM, which is specified in the init_func\n",
    "        return output_shape, ()\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        return inputs[0][0] # this is h_final, look at the actual output by lstm (h_final, m_final), output\n",
    "\n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### next step we want to generate some inputs for LSTM layer for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import value_and_grad, jit,grad\n",
    "from jax.experimental import stax\n",
    "from patch_gnn.training import mseloss\n",
    "from jax.experimental.optimizers import adam\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize the network and perform a forward pass\n",
    "vanilla_lstm_init_fun, vanilla_lstm_apply_fun = stax.serial(\n",
    "    AAEmbedding(17), \n",
    "    stax.Sigmoid, \n",
    "    LSTM(32),\n",
    "    LSTMHiddenStates(),\n",
    "    #stax.Relu, don't use relu given the output are mostly negatives\n",
    "    stax.Dense(1))#Dense(lstm_train_oh.shape[0])\n",
    "\n",
    "#vanilla_lstm_init_fun, vanilla_lstm_apply_fun = LSTM(32)\n",
    "_, params = vanilla_lstm_init_fun(key, (lstm_train_oh.shape[0], padding_length, 21)) # params here include params of all layers\n",
    "\n",
    "\n",
    "def mse_loss(params,predict, inputs, targets):\n",
    "#    \"\"\" Calculate the Mean Squared Error Prediction Loss. \"\"\"\n",
    "    preds = predict(params, inputs)\n",
    "#    preds = vmap(partial(predict, params))(inputs)\n",
    "    return np.mean((preds - targets)**2)\n",
    "# the following update function is the same as step function in patch_gnn.training\n",
    "def fit(\n",
    "    lstm_train_oh ,\n",
    "    train_target ,\n",
    "    num_epochs: int,\n",
    "    params, #default for params is the pre-trained weights\n",
    "    #batch_size: int = 25,\n",
    "    #holdout_seqs: Optional[Iterable[str]] = None,\n",
    "    #proj_name: str = \"temp\",\n",
    "    #epochs_per_print: int = 1,\n",
    "    step_size: float = 0.0001,\n",
    "    model_func = vanilla_lstm_apply_fun,\n",
    "    backend: str = \"cpu\",\n",
    "    \n",
    "):\n",
    "    \"\"\"referencing unirep https://github.com/ElArkk/jax-unirep/blob/12209f58165daac30fe0d8e8ccece9d9cdec02e7/jax_unirep/evotuning.py fit function\"\"\"\n",
    "    @jit\n",
    "    def step(i, x, y, state):\n",
    "        \"\"\" Perform one timestep forward pass of all layers, calculate the MSE & perform a SGD step. \"\"\"\n",
    "        params = get_params(state)\n",
    "        g = grad(partial(mse_loss, predict = model_func))(params, inputs= x, targets = y)\n",
    "        state = update(i, g, state) # update the state based on gradient using adam\n",
    "        return state  # states have params in a form adam can use\n",
    " \n",
    "    ########### tell adam about the parameters to optimize, this initlize params for step function\n",
    "    init, update, get_params = adam(step_size=step_size)\n",
    "    get_params = jit(get_params)\n",
    "    state = init(params)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        loss_within_epoch = []\n",
    "        for batch_idx in tqdm(range(lstm_train_oh.shape[0])):\n",
    "            x_in = lstm_train_oh[batch_idx, :, :]\n",
    "            y = train_target[batch_idx:batch_idx+1, ]\n",
    "            for timestep in range(padding_length):\n",
    "            #print(f\"x_in shape is {x_in.shape},y shape is {y.shape}\")\n",
    "                state = step(timestep, x_in, y, state)\n",
    "                loss = mse_loss(get_params(state),model_func, x_in, y)\n",
    "                \n",
    "                # get the loss of last time point\n",
    "                if timestep == (padding_length-1):\n",
    "                    # calculate within epoch loss avg\n",
    "                    loss_within_epoch.append(loss)\n",
    "                    \n",
    "                    print(f\"loss for time step {timestep} is {loss}\")\n",
    "        avg_loss_in_epoch = np.mean(np.array(loss_within_epoch))\n",
    "        print(f\"average loss for this epoch is {avg_loss_in_epoch}\")\n",
    "    return get_params(state)  # in the end, get the params from the final state\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tuned_params, test_set_x, test_target, model_apply_fun ):\n",
    "    \"\"\"\n",
    "    predict the output for test set and calculate the loss, returns pred_test: a list, and mse_loss: a float, and explained_var_score: int\n",
    "    :param tuned_params: the tuned params from training by calling the `fit` function\n",
    "    :param test_set_x: same shape as train set of shape (n_samples, padding_length, n_unique_letters)\n",
    "    :param test_target: same shape as train_target, of shape (n_samples, )\n",
    "    :param model_apply_fun: the apply_fun of the jax.serial model, e.g vanilla_lstm_apply_fun\n",
    "    \"\"\"\n",
    "    pred_test = []\n",
    "    for batch_idx in tqdm(range(test_set_x.shape[0])):\n",
    "        pred_test_idx = model_apply_fun(tuned_params, test_set_x[batch_idx,:,: ]) \n",
    "        pred_test.append(pred_test_idx)\n",
    "    # reshape pred_test to ndarray\n",
    "    pred_test = np.array(pred_test).reshape(len(pred_test),)\n",
    "    mse_loss = np.mean((pred_test - test_target)**2)\n",
    "    # explained_var_score from https://scikit-learn.org/stable/modules/model_evaluation.html#explained-variance-score\n",
    "    explained_var_score = 1- (np.var(test_target - pred_test)/(np.var(test_target)))\n",
    "    return pred_test, mse_loss, explained_var_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If the inputs in LSTM `apply_fun` is reshaped, the dimension problem will be solved but there will be another error\n",
    "###### input: (50,21) --> AAembedding --> (50, 17) --> (1,17) x 50 time steps --> LSTM --> (1,32) after 50 timesteps\n",
    "### The model takes a lot of time to train, each epoch takes 31 min on avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e359c34c50e74997920890239f1d242b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1851f42243403882d2ab91fa4655f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for time step 49 is 17.385520935058594\n",
      "loss for time step 49 is 0.6765739917755127\n",
      "loss for time step 49 is 10.402125358581543\n",
      "loss for time step 49 is 2.1415634155273438\n",
      "loss for time step 49 is 0.4177894592285156\n",
      "loss for time step 49 is 1.8039796352386475\n",
      "loss for time step 49 is 1.7699885368347168\n",
      "loss for time step 49 is 4.36460542678833\n",
      "loss for time step 49 is 0.2719963788986206\n",
      "loss for time step 49 is 0.0637504979968071\n",
      "loss for time step 49 is 0.23256538808345795\n",
      "loss for time step 49 is 8.307567596435547\n",
      "loss for time step 49 is 0.019412856549024582\n",
      "loss for time step 49 is 4.224788188934326\n",
      "loss for time step 49 is 0.032604366540908813\n",
      "loss for time step 49 is 6.035412788391113\n",
      "loss for time step 49 is 0.025207258760929108\n",
      "loss for time step 49 is 6.363267311826348e-05\n",
      "loss for time step 49 is 1.0819110870361328\n",
      "loss for time step 49 is 0.049980927258729935\n",
      "loss for time step 49 is 2.3452324867248535\n",
      "loss for time step 49 is 6.022099494934082\n",
      "loss for time step 49 is 1.6483668088912964\n",
      "loss for time step 49 is 0.4534725844860077\n",
      "loss for time step 49 is 1.3710131645202637\n",
      "loss for time step 49 is 0.034005939960479736\n",
      "loss for time step 49 is 0.439498633146286\n",
      "loss for time step 49 is 2.074622631072998\n",
      "loss for time step 49 is 0.8269586563110352\n",
      "loss for time step 49 is 0.8544901609420776\n",
      "loss for time step 49 is 1.6107515096664429\n",
      "loss for time step 49 is 0.0056746117770671844\n",
      "loss for time step 49 is 0.9161168932914734\n",
      "loss for time step 49 is 1.2680273056030273\n",
      "loss for time step 49 is 0.33730584383010864\n",
      "loss for time step 49 is 4.6511077880859375\n",
      "loss for time step 49 is 0.28515541553497314\n",
      "loss for time step 49 is 0.9424068927764893\n",
      "loss for time step 49 is 1.4956778287887573\n",
      "loss for time step 49 is 0.7153482437133789\n",
      "loss for time step 49 is 4.3267059326171875\n",
      "loss for time step 49 is 3.055640697479248\n",
      "loss for time step 49 is 1.899855375289917\n",
      "loss for time step 49 is 0.19056926667690277\n",
      "loss for time step 49 is 2.0171871185302734\n",
      "loss for time step 49 is 2.7129406929016113\n",
      "loss for time step 49 is 0.2239997833967209\n",
      "loss for time step 49 is 4.904326915740967\n",
      "loss for time step 49 is 0.1474033147096634\n",
      "loss for time step 49 is 0.37865063548088074\n",
      "loss for time step 49 is 0.09734580665826797\n",
      "loss for time step 49 is 3.0291402339935303\n",
      "loss for time step 49 is 0.8188967704772949\n",
      "loss for time step 49 is 0.05958019942045212\n",
      "loss for time step 49 is 4.417544841766357\n",
      "loss for time step 49 is 2.368562936782837\n",
      "loss for time step 49 is 0.004382189828902483\n",
      "loss for time step 49 is 0.6079089045524597\n",
      "loss for time step 49 is 0.08657114207744598\n",
      "loss for time step 49 is 0.8911990523338318\n",
      "loss for time step 49 is 0.22242167592048645\n",
      "loss for time step 49 is 4.824679374694824\n",
      "loss for time step 49 is 1.1944169998168945\n",
      "loss for time step 49 is 0.002843294758349657\n",
      "loss for time step 49 is 0.26477736234664917\n",
      "loss for time step 49 is 0.5691690444946289\n",
      "loss for time step 49 is 1.4358088970184326\n",
      "loss for time step 49 is 2.8978636264801025\n",
      "loss for time step 49 is 0.0862906277179718\n",
      "loss for time step 49 is 0.0027733505703508854\n",
      "loss for time step 49 is 0.253229022026062\n",
      "loss for time step 49 is 3.3270747661590576\n",
      "loss for time step 49 is 0.1251896619796753\n",
      "loss for time step 49 is 0.18411006033420563\n",
      "loss for time step 49 is 2.181957483291626\n",
      "loss for time step 49 is 1.10116446018219\n",
      "loss for time step 49 is 1.5033115148544312\n",
      "loss for time step 49 is 1.4950419664382935\n",
      "loss for time step 49 is 0.7238838076591492\n",
      "loss for time step 49 is 5.407719612121582\n",
      "loss for time step 49 is 1.6561148166656494\n",
      "loss for time step 49 is 1.7160630226135254\n",
      "loss for time step 49 is 1.755528450012207\n",
      "loss for time step 49 is 1.5185569524765015\n",
      "loss for time step 49 is 0.6416661143302917\n",
      "loss for time step 49 is 2.481904983520508\n",
      "loss for time step 49 is 2.4475300312042236\n",
      "loss for time step 49 is 0.6881392002105713\n",
      "loss for time step 49 is 3.0097265243530273\n",
      "loss for time step 49 is 0.6543344259262085\n",
      "loss for time step 49 is 0.23924526572227478\n",
      "loss for time step 49 is 0.02548259310424328\n",
      "loss for time step 49 is 3.889788866043091\n",
      "loss for time step 49 is 0.04367148131132126\n",
      "loss for time step 49 is 1.5405700206756592\n",
      "loss for time step 49 is 0.5676745772361755\n",
      "loss for time step 49 is 1.0543131828308105\n",
      "loss for time step 49 is 2.458709478378296\n",
      "loss for time step 49 is 0.6569907069206238\n",
      "loss for time step 49 is 0.5482374429702759\n",
      "loss for time step 49 is 3.575108289718628\n",
      "loss for time step 49 is 0.08043929189443588\n",
      "loss for time step 49 is 0.39085882902145386\n",
      "loss for time step 49 is 1.0287225246429443\n",
      "loss for time step 49 is 1.170844554901123\n",
      "loss for time step 49 is 0.03506654128432274\n",
      "loss for time step 49 is 0.004399252124130726\n",
      "loss for time step 49 is 0.9312600493431091\n",
      "loss for time step 49 is 1.0080393552780151\n",
      "loss for time step 49 is 1.0103410482406616\n",
      "loss for time step 49 is 2.6024184226989746\n",
      "loss for time step 49 is 0.2061847299337387\n",
      "loss for time step 49 is 0.0018241395009681582\n",
      "loss for time step 49 is 0.7728293538093567\n",
      "loss for time step 49 is 1.113042950630188\n",
      "loss for time step 49 is 6.293828964233398\n",
      "loss for time step 49 is 0.838313639163971\n",
      "loss for time step 49 is 1.4874536991119385\n",
      "loss for time step 49 is 0.12010424584150314\n",
      "loss for time step 49 is 3.0794780254364014\n",
      "loss for time step 49 is 0.07540124654769897\n",
      "loss for time step 49 is 0.8479915857315063\n",
      "loss for time step 49 is 1.358606219291687\n",
      "loss for time step 49 is 0.44881385564804077\n",
      "loss for time step 49 is 4.887255668640137\n",
      "loss for time step 49 is 0.01575586572289467\n",
      "loss for time step 49 is 2.6975319385528564\n",
      "loss for time step 49 is 0.8509677648544312\n",
      "loss for time step 49 is 2.720668315887451\n",
      "loss for time step 49 is 2.8280715942382812\n",
      "loss for time step 49 is 0.3877785801887512\n",
      "loss for time step 49 is 0.5532001852989197\n",
      "loss for time step 49 is 0.02146465703845024\n",
      "loss for time step 49 is 0.001639143330976367\n",
      "loss for time step 49 is 1.0722287893295288\n",
      "loss for time step 49 is 0.7668400406837463\n",
      "loss for time step 49 is 0.2984392046928406\n",
      "loss for time step 49 is 3.151205062866211\n",
      "loss for time step 49 is 0.4142034649848938\n",
      "loss for time step 49 is 0.5120236873626709\n",
      "loss for time step 49 is 0.26492536067962646\n",
      "loss for time step 49 is 0.33514904975891113\n",
      "loss for time step 49 is 2.085474729537964\n",
      "loss for time step 49 is 7.06939172744751\n",
      "loss for time step 49 is 0.05189387872815132\n",
      "loss for time step 49 is 0.7860842943191528\n",
      "loss for time step 49 is 1.4194821119308472\n",
      "loss for time step 49 is 0.8705043196678162\n",
      "loss for time step 49 is 0.11082497984170914\n",
      "loss for time step 49 is 0.9370552897453308\n",
      "loss for time step 49 is 0.03856071084737778\n",
      "loss for time step 49 is 1.1225255727767944\n",
      "loss for time step 49 is 3.699085235595703\n",
      "loss for time step 49 is 1.3347512483596802\n",
      "loss for time step 49 is 1.7852578163146973\n",
      "loss for time step 49 is 4.01746940612793\n",
      "loss for time step 49 is 7.283681392669678\n",
      "loss for time step 49 is 0.1295059621334076\n",
      "loss for time step 49 is 0.008690367452800274\n",
      "loss for time step 49 is 0.0037985374219715595\n",
      "loss for time step 49 is 4.674529552459717\n",
      "loss for time step 49 is 0.43229594826698303\n",
      "loss for time step 49 is 0.09693977981805801\n",
      "loss for time step 49 is 6.171514987945557\n",
      "loss for time step 49 is 3.58440899848938\n",
      "loss for time step 49 is 0.34319397807121277\n",
      "loss for time step 49 is 2.736072301864624\n",
      "loss for time step 49 is 1.7442212104797363\n",
      "loss for time step 49 is 2.484153985977173\n",
      "loss for time step 49 is 5.038510799407959\n",
      "loss for time step 49 is 0.10467564314603806\n",
      "loss for time step 49 is 1.2573168277740479\n",
      "loss for time step 49 is 0.06499247252941132\n",
      "loss for time step 49 is 0.547454297542572\n",
      "loss for time step 49 is 2.8963418006896973\n",
      "loss for time step 49 is 0.6269049644470215\n",
      "loss for time step 49 is 0.18792684376239777\n",
      "loss for time step 49 is 0.4689660966396332\n",
      "loss for time step 49 is 0.010533214546740055\n",
      "loss for time step 49 is 2.4612417221069336\n",
      "loss for time step 49 is 0.0014554825611412525\n",
      "loss for time step 49 is 0.013768975622951984\n",
      "loss for time step 49 is 1.6944591999053955\n",
      "loss for time step 49 is 0.05653347820043564\n",
      "loss for time step 49 is 0.16930446028709412\n",
      "loss for time step 49 is 4.440919399261475\n",
      "loss for time step 49 is 0.3674622178077698\n",
      "loss for time step 49 is 2.229037046432495\n",
      "loss for time step 49 is 0.0005941620329394937\n",
      "loss for time step 49 is 0.7679572105407715\n",
      "loss for time step 49 is 2.1201531887054443\n",
      "loss for time step 49 is 0.13411621749401093\n",
      "loss for time step 49 is 4.12565803527832\n",
      "loss for time step 49 is 3.796952486038208\n",
      "loss for time step 49 is 1.328032374382019\n",
      "loss for time step 49 is 0.8369165062904358\n",
      "loss for time step 49 is 0.10151245445013046\n",
      "loss for time step 49 is 2.289259910583496\n",
      "loss for time step 49 is 0.2001722902059555\n",
      "loss for time step 49 is 0.0342581570148468\n",
      "loss for time step 49 is 0.8041350841522217\n",
      "loss for time step 49 is 0.24444741010665894\n",
      "loss for time step 49 is 1.1917403936386108\n",
      "loss for time step 49 is 0.23386195302009583\n",
      "loss for time step 49 is 0.086016446352005\n",
      "loss for time step 49 is 0.9208765029907227\n",
      "loss for time step 49 is 0.7787434458732605\n",
      "loss for time step 49 is 0.9038411378860474\n",
      "loss for time step 49 is 0.12119597941637039\n",
      "loss for time step 49 is 0.637710690498352\n",
      "loss for time step 49 is 0.2285304218530655\n",
      "loss for time step 49 is 0.030234744772315025\n",
      "loss for time step 49 is 4.597815036773682\n",
      "loss for time step 49 is 1.3869386911392212\n",
      "loss for time step 49 is 0.15481877326965332\n",
      "loss for time step 49 is 0.40596702694892883\n",
      "loss for time step 49 is 4.7466607093811035\n",
      "loss for time step 49 is 0.21216851472854614\n",
      "loss for time step 49 is 0.6061469912528992\n",
      "loss for time step 49 is 0.7403984665870667\n",
      "loss for time step 49 is 5.41799259185791\n",
      "loss for time step 49 is 0.11422993987798691\n",
      "loss for time step 49 is 0.9274898767471313\n",
      "loss for time step 49 is 0.2487688958644867\n",
      "loss for time step 49 is 3.0297136306762695\n",
      "loss for time step 49 is 5.645212650299072\n",
      "loss for time step 49 is 0.7262396216392517\n",
      "loss for time step 49 is 0.13322535157203674\n",
      "loss for time step 49 is 0.8116773366928101\n",
      "loss for time step 49 is 1.9444334506988525\n",
      "loss for time step 49 is 1.3677377700805664\n",
      "loss for time step 49 is 0.21839897334575653\n",
      "loss for time step 49 is 3.837502956390381\n",
      "loss for time step 49 is 0.4217832088470459\n",
      "loss for time step 49 is 0.4393172264099121\n",
      "loss for time step 49 is 0.03137131407856941\n",
      "loss for time step 49 is 2.538435459136963\n",
      "loss for time step 49 is 5.205881118774414\n",
      "loss for time step 49 is 2.101353168487549\n",
      "loss for time step 49 is 5.826702117919922\n",
      "loss for time step 49 is 0.16119669377803802\n",
      "loss for time step 49 is 4.750743389129639\n",
      "loss for time step 49 is 0.006833693943917751\n",
      "loss for time step 49 is 0.99000483751297\n",
      "loss for time step 49 is 0.2787665128707886\n",
      "loss for time step 49 is 1.818610429763794\n",
      "loss for time step 49 is 1.6784167289733887\n",
      "loss for time step 49 is 0.37955549359321594\n",
      "loss for time step 49 is 0.19492420554161072\n",
      "loss for time step 49 is 0.06045680120587349\n",
      "loss for time step 49 is 3.7581627368927\n",
      "loss for time step 49 is 0.31026074290275574\n",
      "loss for time step 49 is 0.22770406305789948\n",
      "loss for time step 49 is 0.0019972079899162054\n",
      "loss for time step 49 is 0.12952637672424316\n",
      "loss for time step 49 is 0.5241397023200989\n",
      "loss for time step 49 is 0.19279219210147858\n",
      "loss for time step 49 is 0.35424181818962097\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d710b0831d845ce9106e8ae92e18be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for time step 49 is 4.70387077331543\n",
      "loss for time step 49 is 1.0506112575531006\n",
      "loss for time step 49 is 2.2751007080078125\n",
      "loss for time step 49 is 0.014551027677953243\n",
      "loss for time step 49 is 0.6430402398109436\n",
      "loss for time step 49 is 0.0013195560313761234\n",
      "loss for time step 49 is 0.027071937918663025\n",
      "loss for time step 49 is 1.113188624382019\n",
      "loss for time step 49 is 2.163701295852661\n",
      "loss for time step 49 is 1.2698752880096436\n",
      "loss for time step 49 is 0.09711727499961853\n",
      "loss for time step 49 is 4.6928629875183105\n",
      "loss for time step 49 is 0.2664697766304016\n",
      "loss for time step 49 is 2.149998664855957\n",
      "loss for time step 49 is 0.520014226436615\n",
      "loss for time step 49 is 3.9170093536376953\n",
      "loss for time step 49 is 0.07526277750730515\n",
      "loss for time step 49 is 0.15227995812892914\n",
      "loss for time step 49 is 2.0168299674987793\n",
      "loss for time step 49 is 0.015986783429980278\n",
      "loss for time step 49 is 3.461209535598755\n",
      "loss for time step 49 is 4.623901844024658\n",
      "loss for time step 49 is 1.021848440170288\n",
      "loss for time step 49 is 0.863670825958252\n",
      "loss for time step 49 is 0.8848990797996521\n",
      "loss for time step 49 is 0.1575312614440918\n",
      "loss for time step 49 is 0.74711012840271\n",
      "loss for time step 49 is 1.5939160585403442\n",
      "loss for time step 49 is 1.1620535850524902\n",
      "loss for time step 49 is 1.1758195161819458\n",
      "loss for time step 49 is 1.263108253479004\n",
      "loss for time step 49 is 0.043495580554008484\n",
      "loss for time step 49 is 1.1715270280838013\n",
      "loss for time step 49 is 1.0240814685821533\n",
      "loss for time step 49 is 0.47136402130126953\n",
      "loss for time step 49 is 4.260295391082764\n",
      "loss for time step 49 is 0.3847082853317261\n",
      "loss for time step 49 is 1.1103777885437012\n",
      "loss for time step 49 is 1.318693995475769\n",
      "loss for time step 49 is 0.839388906955719\n",
      "loss for time step 49 is 4.0764055252075195\n",
      "loss for time step 49 is 3.2681000232696533\n",
      "loss for time step 49 is 2.052957057952881\n",
      "loss for time step 49 is 0.2344532161951065\n",
      "loss for time step 49 is 2.108462333679199\n",
      "loss for time step 49 is 2.578342914581299\n",
      "loss for time step 49 is 0.26134195923805237\n",
      "loss for time step 49 is 4.717369079589844\n",
      "loss for time step 49 is 0.11792781949043274\n",
      "loss for time step 49 is 0.33394792675971985\n",
      "loss for time step 49 is 0.12033695727586746\n",
      "loss for time step 49 is 2.9232594966888428\n",
      "loss for time step 49 is 0.7732781171798706\n",
      "loss for time step 49 is 0.07180681824684143\n",
      "loss for time step 49 is 4.5263671875\n",
      "loss for time step 49 is 2.438169240951538\n",
      "loss for time step 49 is 0.0021084616892039776\n",
      "loss for time step 49 is 0.6331748366355896\n",
      "loss for time step 49 is 0.0940449982881546\n",
      "loss for time step 49 is 0.8981457352638245\n",
      "loss for time step 49 is 0.21666905283927917\n",
      "loss for time step 49 is 4.74808406829834\n",
      "loss for time step 49 is 1.15397047996521\n",
      "loss for time step 49 is 0.0012861833674833179\n",
      "loss for time step 49 is 0.28146281838417053\n",
      "loss for time step 49 is 0.546627938747406\n",
      "loss for time step 49 is 1.4678869247436523\n",
      "loss for time step 49 is 2.851874351501465\n",
      "loss for time step 49 is 0.07909351587295532\n",
      "loss for time step 49 is 0.001694095553830266\n",
      "loss for time step 49 is 0.2642677426338196\n",
      "loss for time step 49 is 3.294476270675659\n",
      "loss for time step 49 is 0.12021474540233612\n",
      "loss for time step 49 is 0.17956197261810303\n",
      "loss for time step 49 is 2.204129695892334\n",
      "loss for time step 49 is 1.0882662534713745\n",
      "loss for time step 49 is 1.519964337348938\n",
      "loss for time step 49 is 1.5101467370986938\n",
      "loss for time step 49 is 0.7134453654289246\n",
      "loss for time step 49 is 5.421507358551025\n",
      "loss for time step 49 is 1.6463942527770996\n",
      "loss for time step 49 is 1.6729916334152222\n",
      "loss for time step 49 is 1.7691890001296997\n",
      "loss for time step 49 is 1.4912036657333374\n",
      "loss for time step 49 is 0.6508841514587402\n",
      "loss for time step 49 is 2.429265022277832\n",
      "loss for time step 49 is 2.2944328784942627\n",
      "loss for time step 49 is 0.5257413983345032\n",
      "loss for time step 49 is 3.1202619075775146\n",
      "loss for time step 49 is 0.6778678297996521\n",
      "loss for time step 49 is 0.22408024966716766\n",
      "loss for time step 49 is 0.03007894568145275\n",
      "loss for time step 49 is 3.8871212005615234\n",
      "loss for time step 49 is 0.04442394897341728\n",
      "loss for time step 49 is 1.528975009918213\n",
      "loss for time step 49 is 0.5699078440666199\n",
      "loss for time step 49 is 1.0459554195404053\n",
      "loss for time step 49 is 2.4524426460266113\n",
      "loss for time step 49 is 0.6430619955062866\n",
      "loss for time step 49 is 0.5256476998329163\n",
      "loss for time step 49 is 3.5805389881134033\n",
      "loss for time step 49 is 0.0792146548628807\n",
      "loss for time step 49 is 0.3900963366031647\n",
      "loss for time step 49 is 1.0215665102005005\n",
      "loss for time step 49 is 1.1696988344192505\n",
      "loss for time step 49 is 0.03532193601131439\n",
      "loss for time step 49 is 0.004298345651477575\n",
      "loss for time step 49 is 0.9184768795967102\n",
      "loss for time step 49 is 0.9757081270217896\n",
      "loss for time step 49 is 0.9498150944709778\n",
      "loss for time step 49 is 2.35254168510437\n",
      "loss for time step 49 is 0.09688307344913483\n",
      "loss for time step 49 is 0.004681482911109924\n",
      "loss for time step 49 is 0.8114758729934692\n",
      "loss for time step 49 is 1.0272266864776611\n",
      "loss for time step 49 is 6.274743556976318\n",
      "loss for time step 49 is 0.8408453464508057\n",
      "loss for time step 49 is 1.4662472009658813\n",
      "loss for time step 49 is 0.11208035051822662\n",
      "loss for time step 49 is 3.0092501640319824\n",
      "loss for time step 49 is 0.06418345123529434\n",
      "loss for time step 49 is 0.8774298429489136\n",
      "loss for time step 49 is 1.3153761625289917\n",
      "loss for time step 49 is 0.4216039180755615\n",
      "loss for time step 49 is 4.772614002227783\n",
      "loss for time step 49 is 0.022617843002080917\n",
      "loss for time step 49 is 2.6128013134002686\n",
      "loss for time step 49 is 0.89408278465271\n",
      "loss for time step 49 is 2.7782468795776367\n",
      "loss for time step 49 is 2.7638258934020996\n",
      "loss for time step 49 is 0.4095783233642578\n",
      "loss for time step 49 is 0.574007511138916\n",
      "loss for time step 49 is 0.02509707398712635\n",
      "loss for time step 49 is 0.002622139174491167\n",
      "loss for time step 49 is 1.046107530593872\n",
      "loss for time step 49 is 0.7425175309181213\n",
      "loss for time step 49 is 0.2833245098590851\n",
      "loss for time step 49 is 3.1020922660827637\n",
      "loss for time step 49 is 0.39906933903694153\n",
      "loss for time step 49 is 0.49946579337120056\n",
      "loss for time step 49 is 0.2594042122364044\n",
      "loss for time step 49 is 0.34329789876937866\n",
      "loss for time step 49 is 2.112593650817871\n",
      "loss for time step 49 is 7.100976943969727\n",
      "loss for time step 49 is 0.04970945790410042\n",
      "loss for time step 49 is 0.7897065281867981\n",
      "loss for time step 49 is 1.4109737873077393\n",
      "loss for time step 49 is 0.8519696593284607\n",
      "loss for time step 49 is 0.11627130210399628\n",
      "loss for time step 49 is 0.9100841283798218\n",
      "loss for time step 49 is 0.03243569657206535\n",
      "loss for time step 49 is 1.1350189447402954\n",
      "loss for time step 49 is 3.627326011657715\n",
      "loss for time step 49 is 1.2496660947799683\n",
      "loss for time step 49 is 1.8318593502044678\n",
      "loss for time step 49 is 3.8808085918426514\n",
      "loss for time step 49 is 6.313999652862549\n",
      "loss for time step 49 is 0.2794838547706604\n",
      "loss for time step 49 is 0.046087637543678284\n",
      "loss for time step 49 is 0.0011166551848873496\n",
      "loss for time step 49 is 4.725410461425781\n",
      "loss for time step 49 is 0.43101397156715393\n",
      "loss for time step 49 is 0.09427984058856964\n",
      "loss for time step 49 is 6.081620693206787\n",
      "loss for time step 49 is 3.4906070232391357\n",
      "loss for time step 49 is 0.3703880310058594\n",
      "loss for time step 49 is 2.788466691970825\n",
      "loss for time step 49 is 1.7641427516937256\n",
      "loss for time step 49 is 2.4710168838500977\n",
      "loss for time step 49 is 5.004436492919922\n",
      "loss for time step 49 is 0.10988248884677887\n",
      "loss for time step 49 is 1.2604097127914429\n",
      "loss for time step 49 is 0.06428291648626328\n",
      "loss for time step 49 is 0.5429323315620422\n",
      "loss for time step 49 is 2.8747756481170654\n",
      "loss for time step 49 is 0.6096331477165222\n",
      "loss for time step 49 is 0.19592051208019257\n",
      "loss for time step 49 is 0.475701242685318\n",
      "loss for time step 49 is 0.011201064102351665\n",
      "loss for time step 49 is 2.4296135902404785\n",
      "loss for time step 49 is 0.0023636247497051954\n",
      "loss for time step 49 is 0.015998180955648422\n",
      "loss for time step 49 is 1.661804437637329\n",
      "loss for time step 49 is 0.05054366588592529\n",
      "loss for time step 49 is 0.17841178178787231\n",
      "loss for time step 49 is 4.379601001739502\n",
      "loss for time step 49 is 0.34935522079467773\n",
      "loss for time step 49 is 2.1819043159484863\n",
      "loss for time step 49 is 9.033206151798368e-05\n",
      "loss for time step 49 is 0.7904542088508606\n",
      "loss for time step 49 is 2.147329568862915\n",
      "loss for time step 49 is 0.12785284221172333\n",
      "loss for time step 49 is 4.080097675323486\n",
      "loss for time step 49 is 3.829503297805786\n",
      "loss for time step 49 is 1.33845055103302\n",
      "loss for time step 49 is 0.8263862133026123\n",
      "loss for time step 49 is 0.10475927591323853\n",
      "loss for time step 49 is 2.2900917530059814\n",
      "loss for time step 49 is 0.19782203435897827\n",
      "loss for time step 49 is 0.03290684521198273\n",
      "loss for time step 49 is 0.7889266610145569\n",
      "loss for time step 49 is 0.25016525387763977\n",
      "loss for time step 49 is 1.165956974029541\n",
      "loss for time step 49 is 0.2422180324792862\n",
      "loss for time step 49 is 0.08075281232595444\n",
      "loss for time step 49 is 0.8868715763092041\n",
      "loss for time step 49 is 0.7231616377830505\n",
      "loss for time step 49 is 0.7805683016777039\n",
      "loss for time step 49 is 0.06004807725548744\n",
      "loss for time step 49 is 0.6750049591064453\n",
      "loss for time step 49 is 0.20485539734363556\n",
      "loss for time step 49 is 0.02085084468126297\n",
      "loss for time step 49 is 4.556680679321289\n",
      "loss for time step 49 is 1.3963737487792969\n",
      "loss for time step 49 is 0.1510663777589798\n",
      "loss for time step 49 is 0.4053391218185425\n",
      "loss for time step 49 is 4.685514450073242\n",
      "loss for time step 49 is 0.1966138333082199\n",
      "loss for time step 49 is 0.5771594047546387\n",
      "loss for time step 49 is 0.7657580971717834\n",
      "loss for time step 49 is 5.325265407562256\n",
      "loss for time step 49 is 0.12726248800754547\n",
      "loss for time step 49 is 0.8914722800254822\n",
      "loss for time step 49 is 0.23072972893714905\n",
      "loss for time step 49 is 2.9619524478912354\n",
      "loss for time step 49 is 5.717208385467529\n",
      "loss for time step 49 is 0.7454797029495239\n",
      "loss for time step 49 is 0.12551212310791016\n",
      "loss for time step 49 is 0.7902789115905762\n",
      "loss for time step 49 is 1.9677952527999878\n",
      "loss for time step 49 is 1.344948649406433\n",
      "loss for time step 49 is 0.22642958164215088\n",
      "loss for time step 49 is 3.792555570602417\n",
      "loss for time step 49 is 0.4060574173927307\n",
      "loss for time step 49 is 0.45334768295288086\n",
      "loss for time step 49 is 0.028034616261720657\n",
      "loss for time step 49 is 2.557924509048462\n",
      "loss for time step 49 is 5.197874546051025\n",
      "loss for time step 49 is 2.095729112625122\n",
      "loss for time step 49 is 5.798740863800049\n",
      "loss for time step 49 is 0.1657404750585556\n",
      "loss for time step 49 is 4.736227989196777\n",
      "loss for time step 49 is 0.007539763115346432\n",
      "loss for time step 49 is 0.9924184083938599\n",
      "loss for time step 49 is 0.2765915095806122\n",
      "loss for time step 49 is 1.8036898374557495\n",
      "loss for time step 49 is 1.68589186668396\n",
      "loss for time step 49 is 0.37520816922187805\n",
      "loss for time step 49 is 0.1971263736486435\n",
      "loss for time step 49 is 0.061117276549339294\n",
      "loss for time step 49 is 3.735375165939331\n",
      "loss for time step 49 is 0.3160918354988098\n",
      "loss for time step 49 is 0.23107171058654785\n",
      "loss for time step 49 is 0.00226233690045774\n",
      "loss for time step 49 is 0.13073734939098358\n",
      "loss for time step 49 is 0.5228549242019653\n",
      "loss for time step 49 is 0.19013112783432007\n",
      "loss for time step 49 is 0.3551420569419861\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc15c4745c3404590cc8a5fd88a35a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for time step 49 is 4.679271697998047\n",
      "loss for time step 49 is 1.059089183807373\n",
      "loss for time step 49 is 2.255720615386963\n",
      "loss for time step 49 is 0.016113948076963425\n",
      "loss for time step 49 is 0.6497968435287476\n",
      "loss for time step 49 is 0.0010733363451436162\n",
      "loss for time step 49 is 0.02589091844856739\n",
      "loss for time step 49 is 1.101568341255188\n",
      "loss for time step 49 is 2.1713778972625732\n",
      "loss for time step 49 is 1.266916275024414\n",
      "loss for time step 49 is 0.09521597623825073\n",
      "loss for time step 49 is 4.67894983291626\n",
      "loss for time step 49 is 0.2695111632347107\n",
      "loss for time step 49 is 2.1344470977783203\n",
      "loss for time step 49 is 0.5260823369026184\n",
      "loss for time step 49 is 3.890307903289795\n",
      "loss for time step 49 is 0.07885445654392242\n",
      "loss for time step 49 is 0.15650413930416107\n",
      "loss for time step 49 is 2.023925542831421\n",
      "loss for time step 49 is 0.016356611624360085\n",
      "loss for time step 49 is 3.448570728302002\n",
      "loss for time step 49 is 4.617785930633545\n",
      "loss for time step 49 is 1.013806939125061\n",
      "loss for time step 49 is 0.8684021234512329\n",
      "loss for time step 49 is 0.8781352639198303\n",
      "loss for time step 49 is 0.15993714332580566\n",
      "loss for time step 49 is 0.7489734292030334\n",
      "loss for time step 49 is 1.5864237546920776\n",
      "loss for time step 49 is 1.1649632453918457\n",
      "loss for time step 49 is 1.17246413230896\n",
      "loss for time step 49 is 1.261695384979248\n",
      "loss for time step 49 is 0.043774377554655075\n",
      "loss for time step 49 is 1.1672414541244507\n",
      "loss for time step 49 is 1.0238150358200073\n",
      "loss for time step 49 is 0.47007209062576294\n",
      "loss for time step 49 is 4.247603416442871\n",
      "loss for time step 49 is 0.38793134689331055\n",
      "loss for time step 49 is 1.110751748085022\n",
      "loss for time step 49 is 1.3141602277755737\n",
      "loss for time step 49 is 0.8404503464698792\n",
      "loss for time step 49 is 4.061362266540527\n",
      "loss for time step 49 is 3.2730050086975098\n",
      "loss for time step 49 is 2.0461809635162354\n",
      "loss for time step 49 is 0.23024292290210724\n",
      "loss for time step 49 is 2.077273368835449\n",
      "loss for time step 49 is 2.5905070304870605\n",
      "loss for time step 49 is 0.2574349641799927\n",
      "loss for time step 49 is 4.711343765258789\n",
      "loss for time step 49 is 0.11607740074396133\n",
      "loss for time step 49 is 0.32986217737197876\n",
      "loss for time step 49 is 0.1223812997341156\n",
      "loss for time step 49 is 2.905467987060547\n",
      "loss for time step 49 is 0.7618703246116638\n",
      "loss for time step 49 is 0.07502775639295578\n",
      "loss for time step 49 is 4.537052631378174\n",
      "loss for time step 49 is 2.4349114894866943\n",
      "loss for time step 49 is 0.002251621801406145\n",
      "loss for time step 49 is 0.6279525756835938\n",
      "loss for time step 49 is 0.09135356545448303\n",
      "loss for time step 49 is 0.8827347755432129\n",
      "loss for time step 49 is 0.2223106026649475\n",
      "loss for time step 49 is 4.743316173553467\n",
      "loss for time step 49 is 1.1461808681488037\n",
      "loss for time step 49 is 0.0010292446240782738\n",
      "loss for time step 49 is 0.2843116819858551\n",
      "loss for time step 49 is 0.5415058732032776\n",
      "loss for time step 49 is 1.471537470817566\n",
      "loss for time step 49 is 2.8394947052001953\n",
      "loss for time step 49 is 0.0766540989279747\n",
      "loss for time step 49 is 0.0013666501035913825\n",
      "loss for time step 49 is 0.2674398422241211\n",
      "loss for time step 49 is 3.274963140487671\n",
      "loss for time step 49 is 0.11612273752689362\n",
      "loss for time step 49 is 0.17446695268154144\n",
      "loss for time step 49 is 2.2156081199645996\n",
      "loss for time step 49 is 1.0787180662155151\n",
      "loss for time step 49 is 1.5269633531570435\n",
      "loss for time step 49 is 1.5107743740081787\n",
      "loss for time step 49 is 0.7114092111587524\n",
      "loss for time step 49 is 5.409651279449463\n",
      "loss for time step 49 is 1.6292294263839722\n",
      "loss for time step 49 is 1.6367578506469727\n",
      "loss for time step 49 is 1.786641240119934\n",
      "loss for time step 49 is 1.467545509338379\n",
      "loss for time step 49 is 0.6608468294143677\n",
      "loss for time step 49 is 2.390570878982544\n",
      "loss for time step 49 is 2.192945718765259\n",
      "loss for time step 49 is 0.4056246876716614\n",
      "loss for time step 49 is 3.1834418773651123\n",
      "loss for time step 49 is 0.6918702125549316\n",
      "loss for time step 49 is 0.21573860943317413\n",
      "loss for time step 49 is 0.03280874341726303\n",
      "loss for time step 49 is 3.8891475200653076\n",
      "loss for time step 49 is 0.044455207884311676\n",
      "loss for time step 49 is 1.5218071937561035\n",
      "loss for time step 49 is 0.5718169212341309\n",
      "loss for time step 49 is 1.043144941329956\n",
      "loss for time step 49 is 2.4478836059570312\n",
      "loss for time step 49 is 0.6353842616081238\n",
      "loss for time step 49 is 0.5134170651435852\n",
      "loss for time step 49 is 3.582848310470581\n",
      "loss for time step 49 is 0.07855306565761566\n",
      "loss for time step 49 is 0.3898741900920868\n",
      "loss for time step 49 is 1.0178881883621216\n",
      "loss for time step 49 is 1.169431209564209\n",
      "loss for time step 49 is 0.03540326654911041\n",
      "loss for time step 49 is 0.004263746086508036\n",
      "loss for time step 49 is 0.9119362831115723\n",
      "loss for time step 49 is 0.957603394985199\n",
      "loss for time step 49 is 0.913814902305603\n",
      "loss for time step 49 is 2.173027992248535\n",
      "loss for time step 49 is 0.040942057967185974\n",
      "loss for time step 49 is 0.014765266329050064\n",
      "loss for time step 49 is 0.8159734010696411\n",
      "loss for time step 49 is 1.004791259765625\n",
      "loss for time step 49 is 6.226568698883057\n",
      "loss for time step 49 is 0.8542460203170776\n",
      "loss for time step 49 is 1.439411997795105\n",
      "loss for time step 49 is 0.1039080023765564\n",
      "loss for time step 49 is 2.9516072273254395\n",
      "loss for time step 49 is 0.056037526577711105\n",
      "loss for time step 49 is 0.9026400446891785\n",
      "loss for time step 49 is 1.2828034162521362\n",
      "loss for time step 49 is 0.40292075276374817\n",
      "loss for time step 49 is 4.702260494232178\n",
      "loss for time step 49 is 0.027417853474617004\n",
      "loss for time step 49 is 2.560833692550659\n",
      "loss for time step 49 is 0.9205095171928406\n",
      "loss for time step 49 is 2.8135311603546143\n",
      "loss for time step 49 is 2.7269229888916016\n",
      "loss for time step 49 is 0.4223986268043518\n",
      "loss for time step 49 is 0.5864611268043518\n",
      "loss for time step 49 is 0.02741248346865177\n",
      "loss for time step 49 is 0.0033241650089621544\n",
      "loss for time step 49 is 1.0316035747528076\n",
      "loss for time step 49 is 0.7290689945220947\n",
      "loss for time step 49 is 0.27467912435531616\n",
      "loss for time step 49 is 3.0648136138916016\n",
      "loss for time step 49 is 0.384690523147583\n",
      "loss for time step 49 is 0.483060359954834\n",
      "loss for time step 49 is 0.2481943964958191\n",
      "loss for time step 49 is 0.3549099266529083\n",
      "loss for time step 49 is 2.131810426712036\n",
      "loss for time step 49 is 7.1075897216796875\n",
      "loss for time step 49 is 0.04944869503378868\n",
      "loss for time step 49 is 0.7880594730377197\n",
      "loss for time step 49 is 1.401687502861023\n",
      "loss for time step 49 is 0.8377647399902344\n",
      "loss for time step 49 is 0.12055522948503494\n",
      "loss for time step 49 is 0.8904649615287781\n",
      "loss for time step 49 is 0.028249595314264297\n",
      "loss for time step 49 is 1.1434197425842285\n",
      "loss for time step 49 is 3.5758914947509766\n",
      "loss for time step 49 is 1.1733273267745972\n",
      "loss for time step 49 is 1.8620035648345947\n",
      "loss for time step 49 is 3.773113965988159\n",
      "loss for time step 49 is 5.757765769958496\n",
      "loss for time step 49 is 0.4019754230976105\n",
      "loss for time step 49 is 0.10503517836332321\n",
      "loss for time step 49 is 0.01603698544204235\n",
      "loss for time step 49 is 4.839348793029785\n",
      "loss for time step 49 is 0.4407690763473511\n",
      "loss for time step 49 is 0.0957687646150589\n",
      "loss for time step 49 is 6.015223503112793\n",
      "loss for time step 49 is 3.416377544403076\n",
      "loss for time step 49 is 0.39280688762664795\n",
      "loss for time step 49 is 2.828693389892578\n",
      "loss for time step 49 is 1.7751679420471191\n",
      "loss for time step 49 is 2.4464468955993652\n",
      "loss for time step 49 is 4.983706474304199\n",
      "loss for time step 49 is 0.11332672834396362\n",
      "loss for time step 49 is 1.2579189538955688\n",
      "loss for time step 49 is 0.06473022699356079\n",
      "loss for time step 49 is 0.5354406237602234\n",
      "loss for time step 49 is 2.860273599624634\n",
      "loss for time step 49 is 0.5961855053901672\n",
      "loss for time step 49 is 0.20216865837574005\n",
      "loss for time step 49 is 0.4800354838371277\n",
      "loss for time step 49 is 0.011547374539077282\n",
      "loss for time step 49 is 2.4045298099517822\n",
      "loss for time step 49 is 0.0032485562842339277\n",
      "loss for time step 49 is 0.01787731982767582\n",
      "loss for time step 49 is 1.6359198093414307\n",
      "loss for time step 49 is 0.04604112356901169\n",
      "loss for time step 49 is 0.18578535318374634\n",
      "loss for time step 49 is 4.333924770355225\n",
      "loss for time step 49 is 0.3364541232585907\n",
      "loss for time step 49 is 2.1475417613983154\n",
      "loss for time step 49 is 2.4245969143521506e-06\n",
      "loss for time step 49 is 0.8066675662994385\n",
      "loss for time step 49 is 2.1673827171325684\n",
      "loss for time step 49 is 0.12338422238826752\n",
      "loss for time step 49 is 4.049307346343994\n",
      "loss for time step 49 is 3.8519256114959717\n",
      "loss for time step 49 is 1.3471338748931885\n",
      "loss for time step 49 is 0.8186922669410706\n",
      "loss for time step 49 is 0.10721993446350098\n",
      "loss for time step 49 is 2.2935431003570557\n",
      "loss for time step 49 is 0.19708022475242615\n",
      "loss for time step 49 is 0.032300107181072235\n",
      "loss for time step 49 is 0.778906524181366\n",
      "loss for time step 49 is 0.2535218298435211\n",
      "loss for time step 49 is 1.1470412015914917\n",
      "loss for time step 49 is 0.2479039430618286\n",
      "loss for time step 49 is 0.07719872146844864\n",
      "loss for time step 49 is 0.8587900996208191\n",
      "loss for time step 49 is 0.6694506406784058\n",
      "loss for time step 49 is 0.6147943139076233\n",
      "loss for time step 49 is 0.018849363550543785\n",
      "loss for time step 49 is 0.7069815993309021\n",
      "loss for time step 49 is 0.18529291450977325\n",
      "loss for time step 49 is 0.01383903343230486\n",
      "loss for time step 49 is 4.499174118041992\n",
      "loss for time step 49 is 1.4148951768875122\n",
      "loss for time step 49 is 0.1449706107378006\n",
      "loss for time step 49 is 0.40876924991607666\n",
      "loss for time step 49 is 4.619871139526367\n",
      "loss for time step 49 is 0.18132032454013824\n",
      "loss for time step 49 is 0.5492639541625977\n",
      "loss for time step 49 is 0.791596531867981\n",
      "loss for time step 49 is 5.240406513214111\n",
      "loss for time step 49 is 0.1398080289363861\n",
      "loss for time step 49 is 0.8605942130088806\n",
      "loss for time step 49 is 0.2160930186510086\n",
      "loss for time step 49 is 2.9092023372650146\n",
      "loss for time step 49 is 5.778323173522949\n",
      "loss for time step 49 is 0.7638323903083801\n",
      "loss for time step 49 is 0.11860162019729614\n",
      "loss for time step 49 is 0.7726032137870789\n",
      "loss for time step 49 is 1.9902448654174805\n",
      "loss for time step 49 is 1.325576663017273\n",
      "loss for time step 49 is 0.23361101746559143\n",
      "loss for time step 49 is 3.760286331176758\n",
      "loss for time step 49 is 0.3955179750919342\n",
      "loss for time step 49 is 0.46316564083099365\n",
      "loss for time step 49 is 0.025850806385278702\n",
      "loss for time step 49 is 2.574127674102783\n",
      "loss for time step 49 is 5.200870037078857\n",
      "loss for time step 49 is 2.0867786407470703\n",
      "loss for time step 49 is 5.786981582641602\n",
      "loss for time step 49 is 0.16764798760414124\n",
      "loss for time step 49 is 4.7188191413879395\n",
      "loss for time step 49 is 0.008300292305648327\n",
      "loss for time step 49 is 0.9977601170539856\n",
      "loss for time step 49 is 0.27344298362731934\n",
      "loss for time step 49 is 1.7921197414398193\n",
      "loss for time step 49 is 1.6942052841186523\n",
      "loss for time step 49 is 0.371137410402298\n",
      "loss for time step 49 is 0.1995832622051239\n",
      "loss for time step 49 is 0.06215619668364525\n",
      "loss for time step 49 is 3.720966100692749\n",
      "loss for time step 49 is 0.3199271261692047\n",
      "loss for time step 49 is 0.23374240100383759\n",
      "loss for time step 49 is 0.0024985172785818577\n",
      "loss for time step 49 is 0.13209567964076996\n",
      "loss for time step 49 is 0.5237334966659546\n",
      "loss for time step 49 is 0.18947218358516693\n",
      "loss for time step 49 is 0.3544740080833435\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df135b00fc424a16bf786e828af04296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for time step 49 is 4.666069030761719\n",
      "loss for time step 49 is 1.0643385648727417\n",
      "loss for time step 49 is 2.246622323989868\n",
      "loss for time step 49 is 0.016856851056218147\n",
      "loss for time step 49 is 0.6533167362213135\n",
      "loss for time step 49 is 0.000951100664678961\n",
      "loss for time step 49 is 0.025301676243543625\n",
      "loss for time step 49 is 1.0966696739196777\n",
      "loss for time step 49 is 2.1753342151641846\n",
      "loss for time step 49 is 1.267270565032959\n",
      "loss for time step 49 is 0.09483027458190918\n",
      "loss for time step 49 is 4.672245979309082\n",
      "loss for time step 49 is 0.27100545167922974\n",
      "loss for time step 49 is 2.128563642501831\n",
      "loss for time step 49 is 0.5283529162406921\n",
      "loss for time step 49 is 3.8787829875946045\n",
      "loss for time step 49 is 0.08043672144412994\n",
      "loss for time step 49 is 0.1580946147441864\n",
      "loss for time step 49 is 2.0257277488708496\n",
      "loss for time step 49 is 0.016446320340037346\n",
      "loss for time step 49 is 3.445176601409912\n",
      "loss for time step 49 is 4.615749835968018\n",
      "loss for time step 49 is 1.0115200281143188\n",
      "loss for time step 49 is 0.8696040511131287\n",
      "loss for time step 49 is 0.876316249370575\n",
      "loss for time step 49 is 0.16054412722587585\n",
      "loss for time step 49 is 0.7493816018104553\n",
      "loss for time step 49 is 1.5846248865127563\n",
      "loss for time step 49 is 1.1654409170150757\n",
      "loss for time step 49 is 1.1717491149902344\n",
      "loss for time step 49 is 1.261522889137268\n",
      "loss for time step 49 is 0.0438104011118412\n",
      "loss for time step 49 is 1.1664224863052368\n",
      "loss for time step 49 is 1.023767352104187\n",
      "loss for time step 49 is 0.46987366676330566\n",
      "loss for time step 49 is 4.245011329650879\n",
      "loss for time step 49 is 0.38848307728767395\n",
      "loss for time step 49 is 1.110718011856079\n",
      "loss for time step 49 is 1.3135802745819092\n",
      "loss for time step 49 is 0.840470016002655\n",
      "loss for time step 49 is 4.058256149291992\n",
      "loss for time step 49 is 3.2728679180145264\n",
      "loss for time step 49 is 2.044739246368408\n",
      "loss for time step 49 is 0.2293749749660492\n",
      "loss for time step 49 is 2.0672597885131836\n",
      "loss for time step 49 is 2.5921127796173096\n",
      "loss for time step 49 is 0.25695857405662537\n",
      "loss for time step 49 is 4.7098822593688965\n",
      "loss for time step 49 is 0.11581858992576599\n",
      "loss for time step 49 is 0.3293130397796631\n",
      "loss for time step 49 is 0.12264467030763626\n",
      "loss for time step 49 is 2.901463270187378\n",
      "loss for time step 49 is 0.7572881579399109\n",
      "loss for time step 49 is 0.07627106457948685\n",
      "loss for time step 49 is 4.5365705490112305\n",
      "loss for time step 49 is 2.4334518909454346\n",
      "loss for time step 49 is 0.0022947031538933516\n",
      "loss for time step 49 is 0.6270222067832947\n",
      "loss for time step 49 is 0.09088706225156784\n",
      "loss for time step 49 is 0.8790403604507446\n",
      "loss for time step 49 is 0.2234528362751007\n",
      "loss for time step 49 is 4.742728233337402\n",
      "loss for time step 49 is 1.1459052562713623\n",
      "loss for time step 49 is 0.0010191119508817792\n",
      "loss for time step 49 is 0.2843691408634186\n",
      "loss for time step 49 is 0.5412876605987549\n",
      "loss for time step 49 is 1.4714940786361694\n",
      "loss for time step 49 is 2.8389933109283447\n",
      "loss for time step 49 is 0.07639042288064957\n",
      "loss for time step 49 is 0.0013284394517540932\n",
      "loss for time step 49 is 0.26753801107406616\n",
      "loss for time step 49 is 3.268519401550293\n",
      "loss for time step 49 is 0.11411739140748978\n",
      "loss for time step 49 is 0.171194925904274\n",
      "loss for time step 49 is 2.217691659927368\n",
      "loss for time step 49 is 1.0751785039901733\n",
      "loss for time step 49 is 1.5269463062286377\n",
      "loss for time step 49 is 1.5080063343048096\n",
      "loss for time step 49 is 0.7128376960754395\n",
      "loss for time step 49 is 5.403873920440674\n",
      "loss for time step 49 is 1.6260958909988403\n",
      "loss for time step 49 is 1.629525899887085\n",
      "loss for time step 49 is 1.7887247800827026\n",
      "loss for time step 49 is 1.4640198945999146\n",
      "loss for time step 49 is 0.6617565155029297\n",
      "loss for time step 49 is 2.382436990737915\n",
      "loss for time step 49 is 2.14054012298584\n",
      "loss for time step 49 is 0.2554462254047394\n",
      "loss for time step 49 is 3.227755546569824\n",
      "loss for time step 49 is 0.7036373615264893\n",
      "loss for time step 49 is 0.2094235122203827\n",
      "loss for time step 49 is 0.03502092882990837\n",
      "loss for time step 49 is 3.90451979637146\n",
      "loss for time step 49 is 0.042965952306985855\n",
      "loss for time step 49 is 1.5141340494155884\n",
      "loss for time step 49 is 0.5757702589035034\n",
      "loss for time step 49 is 1.0489332675933838\n",
      "loss for time step 49 is 2.4416074752807617\n",
      "loss for time step 49 is 0.6311188340187073\n",
      "loss for time step 49 is 0.5070602893829346\n",
      "loss for time step 49 is 3.5855610370635986\n",
      "loss for time step 49 is 0.07879019528627396\n",
      "loss for time step 49 is 0.3894113600254059\n",
      "loss for time step 49 is 1.018628716468811\n",
      "loss for time step 49 is 1.1692177057266235\n",
      "loss for time step 49 is 0.035438086837530136\n",
      "loss for time step 49 is 0.004251922480762005\n",
      "loss for time step 49 is 0.9098769426345825\n",
      "loss for time step 49 is 0.9487985968589783\n",
      "loss for time step 49 is 0.8913464546203613\n",
      "loss for time step 49 is 2.006509304046631\n",
      "loss for time step 49 is 0.027625426650047302\n",
      "loss for time step 49 is 0.025464138016104698\n",
      "loss for time step 49 is 0.8278396129608154\n",
      "loss for time step 49 is 0.9729379415512085\n",
      "loss for time step 49 is 6.200833797454834\n",
      "loss for time step 49 is 0.8608967661857605\n",
      "loss for time step 49 is 1.4234446287155151\n",
      "loss for time step 49 is 0.0989641547203064\n",
      "loss for time step 49 is 2.9156694412231445\n",
      "loss for time step 49 is 0.05134536698460579\n",
      "loss for time step 49 is 0.9189410209655762\n",
      "loss for time step 49 is 1.2631285190582275\n",
      "loss for time step 49 is 0.39275726675987244\n",
      "loss for time step 49 is 4.679605007171631\n",
      "loss for time step 49 is 0.028895244002342224\n",
      "loss for time step 49 is 2.5545437335968018\n",
      "loss for time step 49 is 0.9251685738563538\n",
      "loss for time step 49 is 2.8291285037994385\n",
      "loss for time step 49 is 2.7175183296203613\n",
      "loss for time step 49 is 0.4262916147708893\n",
      "loss for time step 49 is 0.5923264026641846\n",
      "loss for time step 49 is 0.028703218325972557\n",
      "loss for time step 49 is 0.003755678189918399\n",
      "loss for time step 49 is 1.0270130634307861\n",
      "loss for time step 49 is 0.7278647422790527\n",
      "loss for time step 49 is 0.27489960193634033\n",
      "loss for time step 49 is 3.0675413608551025\n",
      "loss for time step 49 is 0.3845021426677704\n",
      "loss for time step 49 is 0.480859637260437\n",
      "loss for time step 49 is 0.24539843201637268\n",
      "loss for time step 49 is 0.35671210289001465\n",
      "loss for time step 49 is 2.129373550415039\n",
      "loss for time step 49 is 7.1043219566345215\n",
      "loss for time step 49 is 0.04962943494319916\n",
      "loss for time step 49 is 0.7880476117134094\n",
      "loss for time step 49 is 1.4010450839996338\n",
      "loss for time step 49 is 0.8331382274627686\n",
      "loss for time step 49 is 0.12169633060693741\n",
      "loss for time step 49 is 0.8802827596664429\n",
      "loss for time step 49 is 0.02576550841331482\n",
      "loss for time step 49 is 1.1437225341796875\n",
      "loss for time step 49 is 3.540071487426758\n",
      "loss for time step 49 is 1.0785163640975952\n",
      "loss for time step 49 is 1.8789455890655518\n",
      "loss for time step 49 is 3.6883392333984375\n",
      "loss for time step 49 is 5.588263034820557\n",
      "loss for time step 49 is 0.4352256953716278\n",
      "loss for time step 49 is 0.1279875785112381\n",
      "loss for time step 49 is 0.026707876473665237\n",
      "loss for time step 49 is 5.038928508758545\n",
      "loss for time step 49 is 0.4639565050601959\n",
      "loss for time step 49 is 0.10210277885198593\n",
      "loss for time step 49 is 5.981272220611572\n",
      "loss for time step 49 is 3.3708643913269043\n",
      "loss for time step 49 is 0.407030314207077\n",
      "loss for time step 49 is 2.8515806198120117\n",
      "loss for time step 49 is 1.7754335403442383\n",
      "loss for time step 49 is 2.4126784801483154\n",
      "loss for time step 49 is 4.9791035652160645\n",
      "loss for time step 49 is 0.11449592560529709\n",
      "loss for time step 49 is 1.250177264213562\n",
      "loss for time step 49 is 0.06627491116523743\n",
      "loss for time step 49 is 0.5262613296508789\n",
      "loss for time step 49 is 2.8544821739196777\n",
      "loss for time step 49 is 0.5877232551574707\n",
      "loss for time step 49 is 0.20597054064273834\n",
      "loss for time step 49 is 0.48152002692222595\n",
      "loss for time step 49 is 0.011534721590578556\n",
      "loss for time step 49 is 2.387810707092285\n",
      "loss for time step 49 is 0.003935331478714943\n",
      "loss for time step 49 is 0.019195519387722015\n",
      "loss for time step 49 is 1.6176003217697144\n",
      "loss for time step 49 is 0.042971983551979065\n",
      "loss for time step 49 is 0.19110272824764252\n",
      "loss for time step 49 is 4.303481578826904\n",
      "loss for time step 49 is 0.3285967707633972\n",
      "loss for time step 49 is 2.1286869049072266\n",
      "loss for time step 49 is 5.791335934191011e-05\n",
      "loss for time step 49 is 0.8158547282218933\n",
      "loss for time step 49 is 2.1812686920166016\n",
      "loss for time step 49 is 0.12034125626087189\n",
      "loss for time step 49 is 4.033606052398682\n",
      "loss for time step 49 is 3.866199254989624\n",
      "loss for time step 49 is 1.3542722463607788\n",
      "loss for time step 49 is 0.812897801399231\n",
      "loss for time step 49 is 0.10915186256170273\n",
      "loss for time step 49 is 2.297210216522217\n",
      "loss for time step 49 is 0.19648456573486328\n",
      "loss for time step 49 is 0.03174496069550514\n",
      "loss for time step 49 is 0.7684503793716431\n",
      "loss for time step 49 is 0.25683626532554626\n",
      "loss for time step 49 is 1.1268041133880615\n",
      "loss for time step 49 is 0.25372451543807983\n",
      "loss for time step 49 is 0.0736503079533577\n",
      "loss for time step 49 is 0.8278090357780457\n",
      "loss for time step 49 is 0.5980202555656433\n",
      "loss for time step 49 is 0.439797967672348\n",
      "loss for time step 49 is 0.0067472681403160095\n",
      "loss for time step 49 is 0.8024061322212219\n",
      "loss for time step 49 is 0.13492298126220703\n",
      "loss for time step 49 is 0.003539580851793289\n",
      "loss for time step 49 is 4.439762115478516\n",
      "loss for time step 49 is 1.4341838359832764\n",
      "loss for time step 49 is 0.13890215754508972\n",
      "loss for time step 49 is 0.41218385100364685\n",
      "loss for time step 49 is 4.555940628051758\n",
      "loss for time step 49 is 0.16694390773773193\n",
      "loss for time step 49 is 0.5226022005081177\n",
      "loss for time step 49 is 0.8169091939926147\n",
      "loss for time step 49 is 5.1592254638671875\n",
      "loss for time step 49 is 0.1524432897567749\n",
      "loss for time step 49 is 0.8321018218994141\n",
      "loss for time step 49 is 0.20323112607002258\n",
      "loss for time step 49 is 2.868438243865967\n",
      "loss for time step 49 is 5.835319519042969\n",
      "loss for time step 49 is 0.7823217511177063\n",
      "loss for time step 49 is 0.11195235699415207\n",
      "loss for time step 49 is 0.7560259699821472\n",
      "loss for time step 49 is 2.013016700744629\n",
      "loss for time step 49 is 1.3066776990890503\n",
      "loss for time step 49 is 0.2408909797668457\n",
      "loss for time step 49 is 3.733475923538208\n",
      "loss for time step 49 is 0.3882910907268524\n",
      "loss for time step 49 is 0.4708850383758545\n",
      "loss for time step 49 is 0.024215759709477425\n",
      "loss for time step 49 is 2.5916495323181152\n",
      "loss for time step 49 is 5.206425189971924\n",
      "loss for time step 49 is 2.0744571685791016\n",
      "loss for time step 49 is 5.773837566375732\n",
      "loss for time step 49 is 0.16973236203193665\n",
      "loss for time step 49 is 4.695003509521484\n",
      "loss for time step 49 is 0.009371365420520306\n",
      "loss for time step 49 is 1.005654215812683\n",
      "loss for time step 49 is 0.269008994102478\n",
      "loss for time step 49 is 1.778014063835144\n",
      "loss for time step 49 is 1.7057780027389526\n",
      "loss for time step 49 is 0.3656982481479645\n",
      "loss for time step 49 is 0.20303845405578613\n",
      "loss for time step 49 is 0.06369031965732574\n",
      "loss for time step 49 is 3.705026388168335\n",
      "loss for time step 49 is 0.32444360852241516\n",
      "loss for time step 49 is 0.23731938004493713\n",
      "loss for time step 49 is 0.002841820241883397\n",
      "loss for time step 49 is 0.13412503898143768\n",
      "loss for time step 49 is 0.5255889296531677\n",
      "loss for time step 49 is 0.18895280361175537\n",
      "loss for time step 49 is 0.35308173298835754\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e227c9ba50ae4b54b19bae4830842f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for time step 49 is 4.652132987976074\n",
      "loss for time step 49 is 1.0712687969207764\n",
      "loss for time step 49 is 2.239184856414795\n",
      "loss for time step 49 is 0.017418138682842255\n",
      "loss for time step 49 is 0.6576134562492371\n",
      "loss for time step 49 is 0.0008014730410650373\n",
      "loss for time step 49 is 0.024602999910712242\n",
      "loss for time step 49 is 1.094420313835144\n",
      "loss for time step 49 is 2.1814537048339844\n",
      "loss for time step 49 is 1.270912528038025\n",
      "loss for time step 49 is 0.09513683617115021\n",
      "loss for time step 49 is 4.661959171295166\n",
      "loss for time step 49 is 0.27349719405174255\n",
      "loss for time step 49 is 2.1255462169647217\n",
      "loss for time step 49 is 0.530413031578064\n",
      "loss for time step 49 is 3.880742311477661\n",
      "loss for time step 49 is 0.08008157461881638\n",
      "loss for time step 49 is 0.15792876482009888\n",
      "loss for time step 49 is 2.0298731327056885\n",
      "loss for time step 49 is 0.016899099573493004\n",
      "loss for time step 49 is 3.4508681297302246\n",
      "loss for time step 49 is 4.6059041023254395\n",
      "loss for time step 49 is 1.009734869003296\n",
      "loss for time step 49 is 0.872643768787384\n",
      "loss for time step 49 is 0.8750012516975403\n",
      "loss for time step 49 is 0.16125814616680145\n",
      "loss for time step 49 is 0.7525840997695923\n",
      "loss for time step 49 is 1.5830413103103638\n",
      "loss for time step 49 is 1.1687005758285522\n",
      "loss for time step 49 is 1.1763505935668945\n",
      "loss for time step 49 is 1.2574975490570068\n",
      "loss for time step 49 is 0.04451112449169159\n",
      "loss for time step 49 is 1.1704480648040771\n",
      "loss for time step 49 is 1.020151972770691\n",
      "loss for time step 49 is 0.4723653495311737\n",
      "loss for time step 49 is 4.243051528930664\n",
      "loss for time step 49 is 0.38938769698143005\n",
      "loss for time step 49 is 1.1145849227905273\n",
      "loss for time step 49 is 1.311312198638916\n",
      "loss for time step 49 is 0.8433979749679565\n",
      "loss for time step 49 is 4.0599045753479\n",
      "loss for time step 49 is 3.2774882316589355\n",
      "loss for time step 49 is 2.049999713897705\n",
      "loss for time step 49 is 0.2301793098449707\n",
      "loss for time step 49 is 2.052398204803467\n",
      "loss for time step 49 is 2.588106870651245\n",
      "loss for time step 49 is 0.2580590546131134\n",
      "loss for time step 49 is 4.6990556716918945\n",
      "loss for time step 49 is 0.11460437625646591\n",
      "loss for time step 49 is 0.3283415138721466\n",
      "loss for time step 49 is 0.12342140823602676\n",
      "loss for time step 49 is 2.9061264991760254\n",
      "loss for time step 49 is 0.7623486518859863\n",
      "loss for time step 49 is 0.07482649385929108\n",
      "loss for time step 49 is 4.5392560958862305\n",
      "loss for time step 49 is 2.441634178161621\n",
      "loss for time step 49 is 0.002058532787486911\n",
      "loss for time step 49 is 0.6301206946372986\n",
      "loss for time step 49 is 0.09156858175992966\n",
      "loss for time step 49 is 0.8748398423194885\n",
      "loss for time step 49 is 0.2240164875984192\n",
      "loss for time step 49 is 4.730441093444824\n",
      "loss for time step 49 is 1.143227458000183\n",
      "loss for time step 49 is 0.0009709231089800596\n",
      "loss for time step 49 is 0.2860082983970642\n",
      "loss for time step 49 is 0.5406293869018555\n",
      "loss for time step 49 is 1.4763892889022827\n",
      "loss for time step 49 is 2.83943772315979\n",
      "loss for time step 49 is 0.07708601653575897\n",
      "loss for time step 49 is 0.0014283409109339118\n",
      "loss for time step 49 is 0.2672731876373291\n",
      "loss for time step 49 is 3.2804477214813232\n",
      "loss for time step 49 is 0.11683586239814758\n",
      "loss for time step 49 is 0.17448130249977112\n",
      "loss for time step 49 is 2.211167812347412\n",
      "loss for time step 49 is 1.0818578004837036\n",
      "loss for time step 49 is 1.523571252822876\n",
      "loss for time step 49 is 1.5116713047027588\n",
      "loss for time step 49 is 0.7121888995170593\n",
      "loss for time step 49 is 5.419026851654053\n",
      "loss for time step 49 is 1.631929874420166\n",
      "loss for time step 49 is 1.6165730953216553\n",
      "loss for time step 49 is 1.7835545539855957\n",
      "loss for time step 49 is 1.4608269929885864\n",
      "loss for time step 49 is 0.6592968106269836\n",
      "loss for time step 49 is 2.3651883602142334\n",
      "loss for time step 49 is 2.004953622817993\n",
      "loss for time step 49 is 0.11504549533128738\n",
      "loss for time step 49 is 3.3173391819000244\n",
      "loss for time step 49 is 0.7179864645004272\n",
      "loss for time step 49 is 0.20106567442417145\n",
      "loss for time step 49 is 0.03794495761394501\n",
      "loss for time step 49 is 3.901602268218994\n",
      "loss for time step 49 is 0.0434718132019043\n",
      "loss for time step 49 is 1.5105253458023071\n",
      "loss for time step 49 is 0.5755578875541687\n",
      "loss for time step 49 is 1.044854998588562\n",
      "loss for time step 49 is 2.441906213760376\n",
      "loss for time step 49 is 0.6256282329559326\n",
      "loss for time step 49 is 0.4957757890224457\n",
      "loss for time step 49 is 3.5825586318969727\n",
      "loss for time step 49 is 0.07750719040632248\n",
      "loss for time step 49 is 0.3908168077468872\n",
      "loss for time step 49 is 1.0135222673416138\n",
      "loss for time step 49 is 1.1716861724853516\n",
      "loss for time step 49 is 0.03506600856781006\n",
      "loss for time step 49 is 0.0043615070171654224\n",
      "loss for time step 49 is 0.9053405523300171\n",
      "loss for time step 49 is 0.9316150546073914\n",
      "loss for time step 49 is 0.8541499376296997\n",
      "loss for time step 49 is 1.8562192916870117\n",
      "loss for time step 49 is 0.030664466321468353\n",
      "loss for time step 49 is 0.03143363445997238\n",
      "loss for time step 49 is 0.9057877659797668\n",
      "loss for time step 49 is 0.8651741743087769\n",
      "loss for time step 49 is 6.268412113189697\n",
      "loss for time step 49 is 0.8335219621658325\n",
      "loss for time step 49 is 1.4355300664901733\n",
      "loss for time step 49 is 0.09918328374624252\n",
      "loss for time step 49 is 2.884061336517334\n",
      "loss for time step 49 is 0.046627819538116455\n",
      "loss for time step 49 is 0.9313334226608276\n",
      "loss for time step 49 is 1.2421932220458984\n",
      "loss for time step 49 is 0.37950703501701355\n",
      "loss for time step 49 is 4.634369373321533\n",
      "loss for time step 49 is 0.03217904642224312\n",
      "loss for time step 49 is 2.5397214889526367\n",
      "loss for time step 49 is 0.9375122785568237\n",
      "loss for time step 49 is 2.860246181488037\n",
      "loss for time step 49 is 2.6931118965148926\n",
      "loss for time step 49 is 0.43609103560447693\n",
      "loss for time step 49 is 0.6044347286224365\n",
      "loss for time step 49 is 0.03122815489768982\n",
      "loss for time step 49 is 0.00462028244510293\n",
      "loss for time step 49 is 1.0153093338012695\n",
      "loss for time step 49 is 0.7219693660736084\n",
      "loss for time step 49 is 0.2737719714641571\n",
      "loss for time step 49 is 3.0903875827789307\n",
      "loss for time step 49 is 0.39764732122421265\n",
      "loss for time step 49 is 0.49908775091171265\n",
      "loss for time step 49 is 0.2591514587402344\n",
      "loss for time step 49 is 0.3433534801006317\n",
      "loss for time step 49 is 2.116929054260254\n",
      "loss for time step 49 is 7.137606143951416\n",
      "loss for time step 49 is 0.04668663069605827\n",
      "loss for time step 49 is 0.8012198209762573\n",
      "loss for time step 49 is 1.4166309833526611\n",
      "loss for time step 49 is 0.8369819521903992\n",
      "loss for time step 49 is 0.11964454501867294\n",
      "loss for time step 49 is 0.8727413415908813\n",
      "loss for time step 49 is 0.023192239925265312\n",
      "loss for time step 49 is 1.1353791952133179\n",
      "loss for time step 49 is 3.496772527694702\n",
      "loss for time step 49 is 0.9192397594451904\n",
      "loss for time step 49 is 1.9058970212936401\n",
      "loss for time step 49 is 3.579986333847046\n",
      "loss for time step 49 is 5.993537425994873\n",
      "loss for time step 49 is 0.3450824022293091\n",
      "loss for time step 49 is 0.09159369021654129\n",
      "loss for time step 49 is 0.014342608861625195\n",
      "loss for time step 49 is 5.21196174621582\n",
      "loss for time step 49 is 0.498823344707489\n",
      "loss for time step 49 is 0.11191198974847794\n",
      "loss for time step 49 is 5.932319641113281\n",
      "loss for time step 49 is 3.3077290058135986\n",
      "loss for time step 49 is 0.4274560213088989\n",
      "loss for time step 49 is 2.884584903717041\n",
      "loss for time step 49 is 1.7745810747146606\n",
      "loss for time step 49 is 2.3626158237457275\n",
      "loss for time step 49 is 4.975199222564697\n",
      "loss for time step 49 is 0.11581452935934067\n",
      "loss for time step 49 is 1.2368944883346558\n",
      "loss for time step 49 is 0.06896530836820602\n",
      "loss for time step 49 is 0.5122635364532471\n",
      "loss for time step 49 is 2.847536325454712\n",
      "loss for time step 49 is 0.5756620764732361\n",
      "loss for time step 49 is 0.2113671600818634\n",
      "loss for time step 49 is 0.4829740524291992\n",
      "loss for time step 49 is 0.011393503285944462\n",
      "loss for time step 49 is 2.3637423515319824\n",
      "loss for time step 49 is 0.005043452139943838\n",
      "loss for time step 49 is 0.021159065887331963\n",
      "loss for time step 49 is 1.5915106534957886\n",
      "loss for time step 49 is 0.03880797699093819\n",
      "loss for time step 49 is 0.19880391657352448\n",
      "loss for time step 49 is 4.2649617195129395\n",
      "loss for time step 49 is 0.32061269879341125\n",
      "loss for time step 49 is 2.1264514923095703\n",
      "loss for time step 49 is 4.6658358769491315e-05\n",
      "loss for time step 49 is 0.8225196003913879\n",
      "loss for time step 49 is 2.204868793487549\n",
      "loss for time step 49 is 0.11524596810340881\n",
      "loss for time step 49 is 4.026590347290039\n",
      "loss for time step 49 is 3.8908050060272217\n",
      "loss for time step 49 is 1.3712644577026367\n",
      "loss for time step 49 is 0.8006908297538757\n",
      "loss for time step 49 is 0.11339640617370605\n",
      "loss for time step 49 is 2.310919761657715\n",
      "loss for time step 49 is 0.19791366159915924\n",
      "loss for time step 49 is 0.03175439313054085\n",
      "loss for time step 49 is 0.7559308409690857\n",
      "loss for time step 49 is 0.25983256101608276\n",
      "loss for time step 49 is 1.0987135171890259\n",
      "loss for time step 49 is 0.2608505189418793\n",
      "loss for time step 49 is 0.06929020583629608\n",
      "loss for time step 49 is 0.7809727191925049\n",
      "loss for time step 49 is 0.5008644461631775\n",
      "loss for time step 49 is 0.45265311002731323\n",
      "loss for time step 49 is 0.012108422815799713\n",
      "loss for time step 49 is 0.9075080752372742\n",
      "loss for time step 49 is 0.10799671709537506\n",
      "loss for time step 49 is 0.001724336063489318\n",
      "loss for time step 49 is 4.445699691772461\n",
      "loss for time step 49 is 1.405137062072754\n",
      "loss for time step 49 is 0.1450481414794922\n",
      "loss for time step 49 is 0.3913058340549469\n",
      "loss for time step 49 is 4.504244804382324\n",
      "loss for time step 49 is 0.15246610343456268\n",
      "loss for time step 49 is 0.4927128255367279\n",
      "loss for time step 49 is 0.8434717655181885\n",
      "loss for time step 49 is 5.060751438140869\n",
      "loss for time step 49 is 0.1687193065881729\n",
      "loss for time step 49 is 0.7975497245788574\n",
      "loss for time step 49 is 0.18805977702140808\n",
      "loss for time step 49 is 2.8304295539855957\n",
      "loss for time step 49 is 5.910915374755859\n",
      "loss for time step 49 is 0.8075742721557617\n",
      "loss for time step 49 is 0.1032867282629013\n",
      "loss for time step 49 is 0.7338469624519348\n",
      "loss for time step 49 is 2.0438270568847656\n",
      "loss for time step 49 is 1.281018614768982\n",
      "loss for time step 49 is 0.25103020668029785\n",
      "loss for time step 49 is 3.699467182159424\n",
      "loss for time step 49 is 0.3814026117324829\n",
      "loss for time step 49 is 0.4804510474205017\n",
      "loss for time step 49 is 0.02226831391453743\n",
      "loss for time step 49 is 2.618680715560913\n",
      "loss for time step 49 is 5.214814186096191\n",
      "loss for time step 49 is 2.0554590225219727\n",
      "loss for time step 49 is 5.754324436187744\n",
      "loss for time step 49 is 0.17267030477523804\n",
      "loss for time step 49 is 4.659820079803467\n",
      "loss for time step 49 is 0.01105110440403223\n",
      "loss for time step 49 is 1.0176409482955933\n",
      "loss for time step 49 is 0.2625260651111603\n",
      "loss for time step 49 is 1.7581621408462524\n",
      "loss for time step 49 is 1.72266685962677\n",
      "loss for time step 49 is 0.357948899269104\n",
      "loss for time step 49 is 0.20807689428329468\n",
      "loss for time step 49 is 0.06595183908939362\n",
      "loss for time step 49 is 3.683358669281006\n",
      "loss for time step 49 is 0.3307768404483795\n",
      "loss for time step 49 is 0.24243809282779694\n",
      "loss for time step 49 is 0.0033679662737995386\n",
      "loss for time step 49 is 0.13703800737857819\n",
      "loss for time step 49 is 0.5283012390136719\n",
      "loss for time step 49 is 0.18835197389125824\n",
      "loss for time step 49 is 0.351092666387558\n"
     ]
    }
   ],
   "source": [
    "tuned_params = fit(lstm_train_oh, train_target, num_epochs=5,  params=params, model_func=vanilla_lstm_apply_fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save tuned params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('aa_embed_17_sigmoid_lstm_32_lstm_hiddenstate_dense_5_epochs_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(tuned_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('aa_embed_17_sigmoid_lstm_32_lstm_hiddenstate_dense_5_epochs_weights.pkl', 'rb') as f:\n",
    "    new_weights = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3979190c9042432a891677f9614a8c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_train, train_mse_loss, train_explained_var_score =  predict(tuned_params=tuned_params, \n",
    "                                                    test_set_x=lstm_train_oh, test_target = train_target, model_apply_fun=vanilla_lstm_apply_fun )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae6eca5996e4f098fd5704459e64975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_test, test_mse_loss, test_explained_var_score =  predict(tuned_params=tuned_params, \n",
    "                                                    test_set_x=lstm_test_oh, test_target = test_target, model_apply_fun=vanilla_lstm_apply_fun )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_y_eq_x(ax):\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "\n",
    "    minval = min(xmin, ymin)\n",
    "    maxval = max(xmax, ymax)\n",
    "\n",
    "    ax.plot([minval, maxval], [minval, maxval])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "        figsize=(10, 5), nrows=1, ncols=2, sharex=True, sharey=True,\n",
    "    )\n",
    "\n",
    "\n",
    "ax[0].scatter(train_target, pred_train)\n",
    "ax[0].set_title(f\"Vanilla_LSTM, Train MSE: {train_mse_loss:.2f},evs: {train_explained_var_score:.2f}\")\n",
    "plot_y_eq_x(ax[0])\n",
    "\n",
    "\n",
    "ax[1].scatter(test_target, pred_test)\n",
    "ax[1].set_title(f\"Vanilla_LSTM, Test MSE: {test_mse_loss:.2f},evs: {test_explained_var_score:.2f}\")\n",
    "plot_y_eq_x(ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "It seems that many people are observing the model predicts the same outputs when using RNN models https://github.com/keras-team/keras/issues/6447, there are several solutions suggested by this github issues and I'm summarizing here:\n",
    " \n",
    "\n",
    "\n",
    "- try reducing batch_size -> the batch size in the LSTM is 1, nothing to reduce any further\n",
    "- figure out if there are NaN values in the dataset --> I looked and didn't find any\n",
    "- Use the right activation function -> Only sigmoid activation was using after the AA_embedding layer, no other non-linear activation function were used after that. But I can try to remove that to see the output\n",
    "- Use fewer layers --> currently, it is already the simplest model\n",
    "- data augmentation, increase the size of the data -> not applicable\n",
    "- scaling inputs --> currently using sigmoid to scale the embedding to (0,1), didn't help\n",
    "- reduce the number of neurons per layer -> currently the embedding layer neuron is 17, and LSTM cells are 32. Might not worth shrinking further\n",
    "\n",
    "#### Next step\n",
    "\n",
    "Please look at notebooks/coxji1/LSTM-learn_from_jax_unirep_code-no_activation.ipynb, I removed the sigmiod activation function and it seems to perform slightly better (at least you'd see more variation of the output, rather than consistant output prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# below code can be useful and was useful at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.optimizers import adam\n",
    "# Defining an optimizer in Jax\n",
    "step_size = 1e-3\n",
    "opt_init, opt_update, get_params = adam(step_size)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the training epochs\n",
    "loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx in range(lstm_train_oh.shape[0]):\n",
    "        x_in = lstm_train_oh[batch_idx, :, :]\n",
    "        y = train_target[batch_idx:batch_idx+1, ]\n",
    "        print(type(x_in), type(y))\n",
    "        loss, grads = value_and_grad(mse_loss)(params, x_in, y)\n",
    "        print(loss, grads)\n",
    "        #params, opt_state, loss = update(params, x_in, y, opt_state)\n",
    "        #train_loss_log.append(loss)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### below part is what we want to do ultimately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import stax\n",
    "from jax.experimental.optimizers import adam\n",
    "class VanillaLSTM:\n",
    "    \"\"\"Vanilla shallow LSTM model in sklearn-compatible format.\n",
    "\n",
    "    Forward direction LSTM + linear regression on top.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        one_hot_encoded_shape,\n",
    "        #node_feature_shape,\n",
    "        #num_adjacency,\n",
    "        num_training_steps: int = 100,\n",
    "        optimizer_step_size=1e-5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param one_hot_encoded_shape: (batch_size, timestep, embedding), timestep is the padding_length\n",
    "        \n",
    "        \"\"\"\n",
    "        model_init_fun, model_apply_fun = stax.serial(\n",
    "            Dense(64), \n",
    "            stax.Sigmoid, \n",
    "            LSTM(32), \n",
    "            Dense(1)\n",
    "        )\n",
    "        self.model_apply_fun = model_apply_fun\n",
    "\n",
    "        self.optimizer = adam(step_size=optimizer_step_size)\n",
    "\n",
    "        output_shape, params = model_init_fun(\n",
    "            PRNGKey(42), input_shape=(*one_hot_encoded_shape)\n",
    "        )\n",
    "\n",
    "        self.params = params\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.state_history = []\n",
    "        self.loss_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit model.\n",
    "\n",
    "        :param X: tuple(timestep, embedding)\n",
    "        :param y: vector(values to predict)\n",
    "        \"\"\"\n",
    "        if len(y.shape) == 1:\n",
    "            y = np.reshape(y, (-1, 1))\n",
    "        init, update, get_params = self.optimizer\n",
    "        training_step = partial(\n",
    "            step,\n",
    "            loss_fun=mseloss,\n",
    "            apply_fun=self.model_apply_fun,\n",
    "            update_fun=update,\n",
    "            get_params=get_params,\n",
    "            inputs=X,\n",
    "            outputs=y,\n",
    "        )\n",
    "        training_step = jit(training_step)\n",
    "\n",
    "        state = init(self.params)\n",
    "\n",
    "        for i in tqdm(range(self.num_training_steps)):\n",
    "            state, loss = training_step(i, state)\n",
    "            self.state_history.append(state)\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "        self.params = get_params(state)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, checkpoint: int = None):\n",
    "        \"\"\"\n",
    "        predict\n",
    "        :param X: tuple(adjacency, node_features)\n",
    "        \"\"\"\n",
    "        params = self.params\n",
    "        if checkpoint:\n",
    "            _, _, get_params = self.optimizer\n",
    "            params = get_params(self.state_history[checkpoint])\n",
    "        return vmap(partial(self.model_apply_fun, params))(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = 50\n",
    "model_vanilla_lstm = VanillaLSTM(\n",
    "    one_hot_encoded_shape = (padding_length, 21)\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "model_vanilla_lstm.fit(lstm_train_oh, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: GRU\n",
    "### Implementation of GRU from https://towardsdatascience.com/getting-started-with-jax-mlps-cnns-rnns-d0bc389bd683\n",
    "##### formula of GRU can be found at https://www.google.com/search?q=GRU+formula&tbm=isch&source=iu&ictx=1&fir=VgQCUBNNXFcaTM%252CXHoefSnHEDF68M%252C_&vet=1&usg=AI4_-kSHwsjZClZeI1h2s23kwDj5ncW6Jg&sa=X&ved=2ahUKEwjcwZaW8vjvAhUHKFkFHc8NC8sQ9QF6BAgNEAE&biw=1680&bih=895#imgrc=VgQCUBNNXFcaTM\n",
    "##### machanism of GRU can be found at https://d2l.ai/chapter_recurrent-modern/gru.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.nn import sigmoid\n",
    "from jax.nn.initializers import glorot_normal, normal\n",
    "\n",
    "from functools import partial\n",
    "from jax import lax\n",
    "\n",
    "def GRU(out_dim, W_init=glorot_normal(), b_init=normal()):\n",
    "    def init_fun(rng, input_shape):\n",
    "        \"\"\" Initialize the GRU layer for stax \"\"\"\n",
    "        # input shape is of (batch_size, num_time_steps, embeddings)\n",
    "        hidden = b_init(rng, (input_shape[0], out_dim))\n",
    "\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        update_W, update_U, update_b = (\n",
    "            W_init(k1, (input_shape[2], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),)\n",
    "\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        reset_W, reset_U, reset_b = (\n",
    "            W_init(k1, (input_shape[2], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),)\n",
    "\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        out_W, out_U, out_b = (\n",
    "            W_init(k1, (input_shape[2], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),)\n",
    "        # Input dim 0 represents the batch dimension\n",
    "        # Input dim 1 represents the time dimension (before scan moveaxis)\n",
    "        output_shape = (input_shape[0], input_shape[1], out_dim)\n",
    "        return (output_shape,\n",
    "            (hidden,\n",
    "             (update_W, update_U, update_b),\n",
    "             (reset_W, reset_U, reset_b),\n",
    "             (out_W, out_U, out_b),),)\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        \"\"\" Loop over the time steps of the input sequence \"\"\"\n",
    "        h = params[0]\n",
    "        \n",
    "        def apply_fun_scan(params, hidden, inp):\n",
    "            \"\"\" Perform single step update of the network \"\"\"\n",
    "            _, (update_W, update_U, update_b), (reset_W, reset_U, reset_b), (\n",
    "                out_W, out_U, out_b) = params\n",
    "\n",
    "            update_gate = sigmoid(np.dot(inp, update_W) +\n",
    "                                  np.dot(hidden, update_U) + update_b)\n",
    "            reset_gate = sigmoid(np.dot(inp, reset_W) +\n",
    "                                 np.dot(hidden, reset_U) + reset_b)\n",
    "            output_gate = np.tanh(np.dot(inp, out_W) \n",
    "                                  + np.dot(np.multiply(reset_gate, hidden), out_U) \n",
    "                                  + out_b)\n",
    "            output = np.multiply(update_gate, hidden) + np.multiply(1-update_gate, output_gate)\n",
    "            hidden = output\n",
    "            return hidden, hidden\n",
    "\n",
    "        # Move the time dimension to position 0\n",
    "        inputs = np.moveaxis(inputs, 1, 0)\n",
    "        f = partial(apply_fun_scan, params)\n",
    "        _, h_new = lax.scan(f, h, inputs)\n",
    "        return h_new\n",
    "\n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = 10              # Number of OU timesteps\n",
    "batch_size = 64            # Batchsize\n",
    "num_hidden_units = 12      # GRU cells in the RNN layer \n",
    "\n",
    "# Initialize the network and perform a forward pass\n",
    "init_fun, gru_rnn = stax.serial(Dense(num_hidden_units), Relu,\n",
    "                                GRU(num_hidden_units), Dense(1))\n",
    "_, params = init_fun(key, (batch_size, num_dims, 1))\n",
    "\n",
    "def mse_loss(params, inputs, targets):\n",
    "    \"\"\" Calculate the Mean Squared Error Prediction Loss. \"\"\"\n",
    "    preds = gru_rnn(params, inputs)\n",
    "    return np.mean((preds - targets)**2)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y, opt_state):\n",
    "    \"\"\" Perform a forward pass, calculate the MSE & perform a SGD step. \"\"\"\n",
    "    loss, grads = value_and_grad(mse_loss)(params, x, y)\n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return get_params(opt_state), opt_state, loss\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patch-gnn",
   "language": "python",
   "name": "patch-gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
