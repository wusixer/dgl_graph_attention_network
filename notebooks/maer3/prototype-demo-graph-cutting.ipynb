{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "jit(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: function, arg1: function, arg2: function, arg3: List[int]) -> jaxlib.xla_extension.jax_jit.CompiledFunction\n\nInvoked with: <function _rfft_transpose at 0x7fcd1c4f73a0>, <function _cpp_jit.<locals>.cache_miss at 0x7fcd1c4f7430>, <function _cpp_jit.<locals>.get_device_info at 0x7fcd1c4f74c0>, <function _cpp_jit.<locals>.get_jax_enable_x64 at 0x7fcd1c4f7550>, <function _cpp_jit.<locals>.get_jax_disable_jit_flag at 0x7fcd1c4f75e0>, (0, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-637a2f62e1f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpatch_gnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextract_neighborhood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_feature_dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyprojroot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mproteingraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_pdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Novartis Pharma AG/projects/patch_gnn/sda-patch-gnn/src/patch_gnn/graph.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/patch-gnn/lib/python3.8/site-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# These submodules are separate because they are in an import cycle with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# jax and rely on the names imported above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/patch-gnn/lib/python3.8/site-packages/jax/image/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# flake8: noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from jax._src.image.scale import (\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mresize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mResizeMethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/patch-gnn/lib/python3.8/site-packages/jax/_src/image/scale.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/patch-gnn/lib/python3.8/site-packages/jax/lax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    322\u001b[0m   \u001b[0mwhile_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m )\n\u001b[0;32m--> 324\u001b[0;31m from jax._src.lax.fft import (\n\u001b[0m\u001b[1;32m    325\u001b[0m   \u001b[0mfft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m   \u001b[0mfft_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/patch-gnn/lib/python3.8/site-packages/jax/_src/lax/fft.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_argnums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_rfft_transpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfft_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m   \u001b[0;31m# The transpose of RFFT can't be expressed only in terms of irfft. Instead of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m   \u001b[0;31m# manually building up larger twiddle matrices (which would increase the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/patch-gnn/lib/python3.8/site-packages/jax/api.py\u001b[0m in \u001b[0;36mjit\u001b[0;34m(fun, static_argnums, device, backend, donate_argnums)\u001b[0m\n\u001b[1;32m    179\u001b[0m   \"\"\"\n\u001b[1;32m    180\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_cpp_jit\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0momnistaging_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_cpp_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_argnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdonate_argnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_python_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_argnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdonate_argnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/patch-gnn/lib/python3.8/site-packages/jax/api.py\u001b[0m in \u001b[0;36m_cpp_jit\u001b[0;34m(fun, static_argnums, device, backend, donate_argnums)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m   \u001b[0mstatic_argnums_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstatic_argnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m   cpp_jitted_f = jax_jit.jit(fun, cache_miss, get_device_info,\n\u001b[0m\u001b[1;32m    368\u001b[0m                              \u001b[0mget_jax_enable_x64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_jax_disable_jit_flag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                              static_argnums_)\n",
      "\u001b[0;31mTypeError\u001b[0m: jit(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: function, arg1: function, arg2: function, arg3: List[int]) -> jaxlib.xla_extension.jax_jit.CompiledFunction\n\nInvoked with: <function _rfft_transpose at 0x7fcd1c4f73a0>, <function _cpp_jit.<locals>.cache_miss at 0x7fcd1c4f7430>, <function _cpp_jit.<locals>.get_device_info at 0x7fcd1c4f74c0>, <function _cpp_jit.<locals>.get_jax_enable_x64 at 0x7fcd1c4f7550>, <function _cpp_jit.<locals>.get_jax_disable_jit_flag at 0x7fcd1c4f75e0>, (0, 2)"
     ]
    }
   ],
   "source": [
    "from patch_gnn.graph import extract_neighborhood, generate_feature_dataframe\n",
    "from pyprojroot import here\n",
    "from proteingraph import read_pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, I will demo how to use the functions provided in `patch_gnn` and other packages\n",
    "to do graph neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in example data\n",
    "\n",
    "This is an example dataset, an HIV Protease, from the PDB.\n",
    "\n",
    "Firstly, we use `proteingraph.read_pdb` to get back a Graph object.\n",
    "\n",
    "_Note: This should later be delegated to `graphein`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiv_graph = read_pdb(here() / \"data/hiv1_homology_model.pdb\")\n",
    "hiv_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly inspect what nodes are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiv_graph.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate all patches from a graph of radius size 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_gnn.graph import generate_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_patches = generate_patches(hiv_graph, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize a few of them, let's look at how they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx.draw(graph_patches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(graph_patches[-10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the node input data\n",
    "\n",
    "We are going to now generate the node feature matrices for each of the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_feats = pd.read_csv(here() / \"data/amino_acid_properties.csv\", index_col=0)\n",
    "aa_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_amino_acid(n, d, aa_feats: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Featurize a single amino acid.\n",
    "    \n",
    "    :param n: Graph node.\n",
    "    :param d: Graph node attributes.\n",
    "    :param aa_feats: Dataframe containing amino acid features.\n",
    "    \"\"\"\n",
    "    aa = d[\"residue_name\"]\n",
    "    feats = pd.Series(aa_feats[aa], name=n)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, collect the node featurization functions into a list.\n",
    "We must use `partial` to enure that each function's signature is limited to `n, d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "feature_funcs = [partial(featurize_amino_acid, aa_feats=aa_feats)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we stack the feature tensors for all graphs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_gnn.graph import stack_feature_tensors\n",
    "Fs = stack_feature_tensors(graph_patches, funcs=feature_funcs)\n",
    "Fs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from patch_gnn.graph import (\n",
    "    identity_matrix, \n",
    "    adjacency_matrix, \n",
    "    laplacian_matrix,\n",
    "    to_adjacency_xarray\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacency tensors\n",
    "\n",
    "Next up, we stack the adjacency tensors together.\n",
    "We are going to use 5 adjacency-like matrices, the 1st-3rd power adjacency matrices,\n",
    "followed by the identity matrix and the graph laplacian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "adjacency_funcs = []\n",
    "for i in range(3):\n",
    "    adjacency_funcs.append(partial(adjacency_matrix, power=i, name=f\"adjacency_{i}\"))\n",
    "adjacency_funcs.extend(\n",
    "    [\n",
    "        identity_matrix,\n",
    "        laplacian_matrix,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As = stack_adjacency_tensors(graph_patches, funcs=adjacency_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we build the neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "\n",
    "from jax import lax, vmap, jit, grad\n",
    "from jax.experimental import stax\n",
    "\n",
    "from patch_gnn.layers import MessagePassing, GraphAverage, GraphSummation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example model that we might write\n",
    "\n",
    "Firstly, we might want a custom graph embedding.\n",
    "\n",
    "Here, what we do is stack together a message passing layer,\n",
    "followed by Dense-Simgoid transformation,\n",
    "followed by a graph summation op,\n",
    "then another linear projection to 256 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomGraphEmbedding(n_output: int):\n",
    "    \"\"\"Return an embedding of a graph in n_output dimensions.\"\"\"\n",
    "    init_fun, apply_fun = stax.serial(\n",
    "        MessagePassing(),\n",
    "        stax.Dense(2048),\n",
    "        stax.Sigmoid,\n",
    "        GraphSummation(),\n",
    "        stax.Dense(n_output),\n",
    "    )\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "embedding_init_fun, embedding_apply_fun = CustomGraphEmbedding(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearRegression(num_outputs):\n",
    "    \"\"\"Linear regression layer.\"\"\"\n",
    "    init_fun, apply_fun = stax.serial(\n",
    "        stax.Dense(num_outputs),\n",
    "    )\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "def LogisticRegression(num_outputs):\n",
    "    \"\"\"Logistic regression layer.\"\"\"\n",
    "    init_fun, apply_fun = stax.serial(\n",
    "        stax.Dense(num_outputs),\n",
    "        stax.Softmax,\n",
    "    )\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "model_init_fun, model_apply_fun = stax.serial(\n",
    "    CustomGraphEmbedding(256),\n",
    "    LinearRegression(1),\n",
    ")\n",
    "\n",
    "output_shape, params = model_init_fun(PRNGKey(42), input_shape=(*Fs[0].shape, As[0].shape[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we pass the data through the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = (As, Fs)\n",
    "out = vmap(partial(model_apply_fun, params))(inputs)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = vmap(partial(embedding_apply_fun, params[0]))(inputs)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try some really dumb learning task, like learning random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "\n",
    "outputs = onp.random.normal(size=(len(graph_patches), 1))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we try to predict these two numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_gnn.training import mseloss\n",
    "\n",
    "dloss = grad(mseloss)\n",
    "mseloss(params, model_apply_fun, inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the loss - it's pretty high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.optimizers import adam\n",
    "from patch_gnn.training import mseloss\n",
    "from jax import grad\n",
    "\n",
    "dmseloss = grad(mseloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from typing import Tuple\n",
    "\n",
    "init, update, get_params = adam(step_size=1e-3)\n",
    "get_params = jit(get_params)\n",
    "state = init(params)\n",
    "\n",
    "random_training_step = partial(step, dloss_fun=dmseloss, apply_fun=model_apply_fun, update_fun=update, get_params=get_params, inputs=inputs, outputs=outputs)\n",
    "random_training_step = jit(random_training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "for i in tqdm(range(1000)):\n",
    "    state = random_training_step(i, state, inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_final = get_params(state)\n",
    "mseloss(params_final, model_apply_fun, inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mseloss(params, model_apply_fun, inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = vmap(partial(model_apply_fun, params_final))(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_preds = vmap(partial(model_apply_fun, params))(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(preds.squeeze(), outputs.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(original_preds.squeeze(), outputs.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are the graphs distinguishable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding for first graph, unoptimized params\n",
    "vmap(partial(embedding_apply_fun, params_final[0]))(inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding for second graph, unoptimized params\n",
    "vmap(partial(embedding_apply_fun, params_final[0]))(inputs)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding for first graph, optimized params\n",
    "vmap(partial(embedding_apply_fun, params_final[0]))(inputs)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding for second graph, optimized params\n",
    "vmap(partial(embedding_apply_fun, params_final[0]))(inputs)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_map, tree_flatten, tree_multimap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr, unflattener = tree_flatten(params)\n",
    "type(unflattener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_diff(a1, a2):\n",
    "    return a1 - a2\n",
    "\n",
    "tree_map(np.mean, tree_multimap(array_diff, params, params_final)), tree_map(np.std, tree_multimap(array_diff, params, params_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAVEYARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"You've hit the graveyard!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_adj = adjacency_matrix(subG)\n",
    "G_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_adj.shape, F.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is message passing in linear algebra form\n",
    "\n",
    "F1 = np.dot(G_adj, F)\n",
    "F1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F2 = np.dot(G_adj, F1)\n",
    "F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F3 = np.dot(G_adj, F2)\n",
    "F3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patch-gnn",
   "language": "python",
   "name": "patch-gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
